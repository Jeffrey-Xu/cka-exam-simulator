[
  {
    "id": "Q1",
    "title": "PersistentVolumeClaim (PVC)",
    "category": "Storage",
    "difficulty": "Medium",
    "task": "The MariaDB Deployment in the mariadb namespace has been accidentally deleted. Please restore the Deployment and ensure data persistence. Follow these steps:",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Flannel",
    "preparation": "# 1) Storage class and namespace already exist\nkubectl get storageclass  # local-path available\nkubectl get ns mariadb    # namespace exists\n\n# 2) PersistentVolume already exists\nkubectl get pv mariadb-pv  # 250Mi, RWO, local-path\n\n# 3) MariaDB Deployment template available at ~/mariadb-deployment.yaml\n# Note: MariaDB image will be pulled from Docker Hub automatically",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Verify environment and existing resources\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get nodes\nNAME              STATUS   ROLES           AGE   VERSION\nec2-34-201-252-187.compute-1.amazonaws.com   Ready    control-plane   15m   v1.28.15\nec2-54-144-18-63.compute-1.amazonaws.com   Ready    <none>          12m   v1.28.15\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pv\nNAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\nmariadb-pv   250Mi      RWO            Retain           Available           local-path              5m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get storageclass\nNAME         PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-path   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  5m\n\n# 3) Create PVC (use vim or nano - exam allows both)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim pvc.yaml\n\n# Press 'i' to enter insert mode, then paste:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mariadb\n  namespace: mariadb\nspec:\n  storageClassName: local-path\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 250Mi\n\n# Press Esc, then :wq to save and exit\n\n# Apply the PVC\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f pvc.yaml\npersistentvolumeclaim/mariadb created\n\n# Verify PVC creation (will be Pending until pod is scheduled)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pvc -n mariadb\nNAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nmariadb   Pending                                      local-path     5s\n\n# 4) Edit the existing MariaDB Deployment file\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim ~/mariadb-deployment.yaml\n\n# The file already exists with basic deployment. Add volumes and volumeMounts:\n# Navigate to the containers section and add volumeMounts after ports:\n        volumeMounts:\n        - name: mariadb-data\n          mountPath: /var/lib/mysql\n\n# Navigate to the end of spec section and add volumes before the last closing brace:\n      volumes:\n      - name: mariadb-data\n        persistentVolumeClaim:\n          claimName: mariadb\n\n# Complete file should look like:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mariadb\n  namespace: mariadb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mariadb\n  template:\n    metadata:\n      labels:\n        app: mariadb\n    spec:\n      containers:\n      - name: mariadb\n        image: mariadb:10.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3306\n        volumeMounts:\n        - name: mariadb-data\n          mountPath: /var/lib/mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"rootpassword\"\n      volumes:\n      - name: mariadb-data\n        persistentVolumeClaim:\n          claimName: mariadb\n\n# Save and exit (:wq)\n\n# 5) Apply the updated Deployment\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f ~/mariadb-deployment.yaml\ndeployment.apps/mariadb created\n\n# 6) Verify deployment is running and stable\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n mariadb\nNAME      READY   UP-TO-DATE   AVAILABLE   AGE\nmariadb   1/1     1            1           30s\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n mariadb\nNAME                       READY   STATUS    RESTARTS   AGE\nmariadb-6789d69fc8-xxxxx   1/1     Running   0          35s\n\n# Verify PVC is now bound\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pvc -n mariadb\nNAME      STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nmariadb   Bound    mariadb-pv   250Mi      RWO            local-path     2m\n\n# 7) Optional: Test database functionality (if time permits)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl exec -n mariadb deployment/mariadb -- mysql -uroot -prootpassword -e \"SELECT VERSION();\"\nVERSION()\n10.5.29-MariaDB-ubu2004\n\n# 8) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ PVC created with correct specifications (mariadb, 250Mi, RWO)\n# ✅ PVC bound to existing PV (mariadb-pv)\n# ✅ MariaDB Deployment updated with persistent volume\n# ✅ Deployment running and stable (1/1 Ready)\n# ✅ Database functional and data will persist across pod restarts",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │     kubelet     │    │      │\n│  │  │  - API Server   │    │              │  │   kube-proxy    │    │      │\n│  │  │  - etcd         │    │              │  │   containerd    │    │      │\n│  │  │  - Scheduler    │    │              │  │                 │    │      │\n│  │  │  - Controller   │    │              │  │                 │    │      │\n│  │  └─────────────────┘    │              │  └─────────────────┘    │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │   kubectl CLI   │    │              │  │  MariaDB Pod    │    │      │\n│  │  │   (exam tool)   │    │              │  │                 │    │      │\n│  │  └─────────────────┘    │              │  │  ┌───────────┐  │    │      │\n│  └─────────────────────────┘              │  │  │ Container │  │    │      │\n│                                           │  │  │MariaDB:10.5│  │    │      │\n│                                           │  │  │Port: 3306 │  │    │      │\n│                                           │  │  └───────────┘  │    │      │\n│                                           │  │        │        │    │      │\n│                                           │  │        ▼        │    │      │\n│                                           │  │  ┌───────────┐  │    │      │\n│                                           │  │  │Volume Mount│  │    │      │\n│                                           │  │  │/var/lib/   │  │    │      │\n│                                           │  │  │   mysql    │  │    │      │\n│                                           │  │  └───────────┘  │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │   Storage Layer     │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │PersistentVolume│  │\n                          │  │  mariadb-pv   │  │\n                          │  │   250Mi RWO   │  │\n                          │  │  local-path   │  │\n                          │  └───────────────┘  │\n                          │          ▲          │\n                          │          │          │\n                          │  ┌───────────────┐  │\n                          │  │      PVC      │  │\n                          │  │   mariadb     │  │\n                          │  │   250Mi RWO   │  │\n                          │  │ (Bound to PV) │  │\n                          │  └───────────────┘  │\n                          │          ▲          │\n                          │          │          │\n                          │  ┌───────────────┐  │\n                          │  │ Host Storage  │  │\n                          │  │/mnt/data/     │  │\n                          │  │  mariadb/     │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. PV/PVC BINDING PROCESS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │PV Available │ -> │PVC Pending  │ -> │PVC Bound    │\n   │(no claim)   │    │(no pod yet) │    │(pod created)│\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. VOLUME BINDING MODES:\n   • Immediate: PVC binds to PV immediately\n   • WaitForFirstConsumer: PVC waits until pod is scheduled\n\n3. ACCESS MODES (CRITICAL FOR EXAM):\n   • RWO (ReadWriteOnce): Single node read/write\n   • ROX (ReadOnlyMany): Multiple nodes read-only  \n   • RWX (ReadWriteMany): Multiple nodes read/write\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 8-12 minutes maximum\n   • Always verify with kubectl get commands\n   • Use kubectl apply -f instead of create\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Wrong namespace specification\n   ❌ Mismatched storage class names\n   ❌ Incorrect volumeMount path\n   ❌ Missing PVC claimName in deployment\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get pv (check STATUS: Available -> Bound)\n   ✅ kubectl get pvc -n mariadb (check STATUS: Pending -> Bound)\n   ✅ kubectl get pods -n mariadb (check STATUS: Running)\n   ✅ kubectl get deployment -n mariadb (check READY: 1/1)\n\n4. KUBECTL SHORTCUTS (SAVE TIME):\n   • k = kubectl (if alias configured)\n   • kubectl get pv,pvc -A (check all at once)\n   • kubectl describe pvc mariadb -n mariadb (troubleshoot binding)\n\n📚 STORAGE DOMAIN KNOWLEDGE (10% of CKA):\n\n• PersistentVolume: Cluster-wide storage resource\n• PersistentVolumeClaim: Namespace-scoped storage request\n• StorageClass: Defines storage provisioning policy\n• Volume Types: hostPath, NFS, cloud storage (EBS, etc.)\n• Reclaim Policies: Retain, Delete, Recycle\n\n🔧 TROUBLESHOOTING TIPS:\n\nPVC Stuck in Pending?\n→ Check: StorageClass exists, PV available, access modes match\n\nPod Stuck in Pending?\n→ Check: PVC bound, node has resources, scheduling constraints\n\nMount Issues?\n→ Check: volumeMount path, PVC claimName, container permissions\n\n💡 EXAM DAY REMINDERS:\n\n1. Read question carefully - note exact names and namespaces\n2. Use vim efficiently: :set paste, i (insert), Esc, :wq\n3. Always verify your work with get commands\n4. If stuck, move on and return later\n5. Partial credit given - complete what you can"
  },
  {
    "id": "Q10",
    "title": "Custom Resource Definitions (CRD)",
    "category": "Troubleshooting",
    "difficulty": "Medium",
    "task": "Verify the cert-manager application that has been deployed to the cluster.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) cert-manager already installed\nkubectl get pods -n cert-manager  # cert-manager components running\nkubectl get crd | grep cert-manager  # 6 CRDs available\n\n# 2) Test resources already created\nkubectl get certificates,issuers -n cert-manager  # example resources exist\n\n# 3) kubectl explain functionality available for all cert-manager CRDs",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Verify cert-manager application deployment\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n cert-manager\nNAME                                      READY   STATUS    RESTARTS   AGE\ncert-manager-7d75f47cc5-2k4vp             1/1     Running   0          5m\ncert-manager-cainjector-c778d44d8-wvc2n   1/1     Running   0          5m\ncert-manager-webhook-55d76f97bb-m98jk     1/1     Running   0          5m\n\n# Verify all cert-manager components are running\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get all -n cert-manager\nNAME                                          READY   STATUS    RESTARTS   AGE\npod/cert-manager-7d75f47cc5-2k4vp             1/1     Running   0          5m\npod/cert-manager-cainjector-c778d44d8-wvc2n   1/1     Running   0          5m\npod/cert-manager-webhook-55d76f97bb-m98jk     1/1     Running   0          5m\n\nNAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/cert-manager           ClusterIP   10.98.96.141    <none>        9402/TCP   5m\nservice/cert-manager-webhook   ClusterIP   10.98.248.125   <none>        443/TCP    5m\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           5m\ndeployment.apps/cert-manager-cainjector   1/1     1            1           5m\ndeployment.apps/cert-manager-webhook      1/1     1            1           5m\n\n# 3) List all cert-manager CRDs\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get crd | grep cert-manager\ncertificaterequests.cert-manager.io                   2025-08-09T10:03:52Z\ncertificates.cert-manager.io                          2025-08-09T10:03:52Z\nchallenges.acme.cert-manager.io                       2025-08-09T10:03:52Z\nclusterissuers.cert-manager.io                        2025-08-09T10:03:52Z\nissuers.cert-manager.io                               2025-08-09T10:03:53Z\norders.acme.cert-manager.io                           2025-08-09T10:03:53Z\n\n# 4) Save cert-manager CRDs to ~/resources.yaml (using default output format)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get crd | grep cert-manager > ~/resources.yaml\n\n# Verify the file was created correctly\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat ~/resources.yaml\ncertificaterequests.cert-manager.io                   2025-08-09T10:03:52Z\ncertificates.cert-manager.io                          2025-08-09T10:03:52Z\nchallenges.acme.cert-manager.io                       2025-08-09T10:03:52Z\nclusterissuers.cert-manager.io                        2025-08-09T10:03:52Z\nissuers.cert-manager.io                               2025-08-09T10:03:53Z\norders.acme.cert-manager.io                           2025-08-09T10:03:53Z\n\n# Check file details\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ ls -la ~/resources.yaml\n-rw-rw-r-- 1 ubuntu ubuntu 450 Aug  9 10:05 /home/ubuntu/resources.yaml\n\n# 5) Extract Certificate.spec.subject documentation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl explain certificate.spec.subject\nGROUP:      cert-manager.io\nKIND:       Certificate\nVERSION:    v1\n\nFIELD: subject <Object>\n\nDESCRIPTION:\n    Requested set of X509 certificate subject attributes. More info:\n    https://datatracker.ietf.org/doc/html/rfc5280#section-4.1.2.6 \n     The common name attribute is specified separately in the `commonName`\n    field. Cannot be set if the `literalSubject` field is set.\n    \nFIELDS:\n  countries\t<[]string>\n    Countries to be used on the Certificate.\n\n  localities\t<[]string>\n    Cities to be used on the Certificate.\n\n  organizationalUnits\t<[]string>\n    Organizational Units to be used on the Certificate.\n\n  organizations\t<[]string>\n    Organizations to be used on the Certificate.\n\n  postalCodes\t<[]string>\n    Postal codes to be used on the Certificate.\n\n  provinces\t<[]string>\n    State/Provinces to be used on the Certificate.\n\n  serialNumber\t<string>\n    Serial number to be used on the Certificate.\n\n  streetAddresses\t<[]string>\n    Street addresses to be used on the Certificate.\n\n# 6) Save Certificate.spec.subject documentation to ~/subject.yaml\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl explain certificate.spec.subject > ~/subject.yaml\n\n# Verify the file was created correctly\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat ~/subject.yaml\nGROUP:      cert-manager.io\nKIND:       Certificate\nVERSION:    v1\n\nFIELD: subject <Object>\n\nDESCRIPTION:\n    Requested set of X509 certificate subject attributes. More info:\n    https://datatracker.ietf.org/doc/html/rfc5280#section-4.1.2.6 \n     The common name attribute is specified separately in the `commonName`\n    field. Cannot be set if the `literalSubject` field is set.\n    \nFIELDS:\n  countries\t<[]string>\n    Countries to be used on the Certificate.\n\n  localities\t<[]string>\n    Cities to be used on the Certificate.\n\n  organizationalUnits\t<[]string>\n    Organizational Units to be used on the Certificate.\n\n  organizations\t<[]string>\n    Organizations to be used on the Certificate.\n\n  postalCodes\t<[]string>\n    Postal codes to be used on the Certificate.\n\n  provinces\t<[]string>\n    State/Provinces to be used on the Certificate.\n\n  serialNumber\t<string>\n    Serial number to be used on the Certificate.\n\n  streetAddresses\t<[]string>\n    Street addresses to be used on the Certificate.\n\n# Check file details\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ ls -la ~/subject.yaml\n-rw-rw-r-- 1 ubuntu ubuntu 1002 Aug  9 10:05 /home/ubuntu/subject.yaml\n\n# 7) Additional verification (optional)\n# Check existing cert-manager resources\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get certificates,issuers -n cert-manager\nNAME                                       READY   SECRET             AGE\ncertificate.cert-manager.io/example-cert   True    example-cert-tls   2m\n\nNAME                                    READY   AGE\nissuer.cert-manager.io/example-issuer   True    2m\n\n# Test kubectl explain on other cert-manager fields\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl explain certificate.spec.issuerRef\nGROUP:      cert-manager.io\nKIND:       Certificate\nVERSION:    v1\n\nFIELD: issuerRef <Object>\n\nDESCRIPTION:\n    Reference to the issuer for this certificate. If the `kind` field is not\n    set, or set to `Issuer`, an Issuer resource with the given name in the\n    same namespace as the Certificate will be used. If the `kind` field is set\n    to `ClusterIssuer`, a ClusterIssuer with the provided name will be used.\n    The `name` field in this stanza is required at all times.\n\n# 8) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ cert-manager application verified and running (3 pods ready)\n# ✅ List of cert-manager CRDs saved to ~/resources.yaml (6 CRDs)\n# ✅ Used kubectl's default output format for resources.yaml\n# ✅ Certificate.spec.subject documentation extracted to ~/subject.yaml\n# ✅ Both files created in home directory with correct content\n# ✅ kubectl explain functionality working correctly for CRD fields",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         CRD & Custom Resource Management                   │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │  cert-manager   │    │      │\n│  │  │  - API Server   │    │              │  │     Pods        │    │      │\n│  │  │  - etcd         │    │              │  │                 │    │      │\n│  │  │  - kubectl CLI  │    │              │  │ ┌─────────────┐ │    │      │\n│  │  └─────────────────┘    │              │  │ │cert-manager │ │    │      │\n│  │                         │              │  │ │ controller  │ │    │      │\n│  │  ┌─────────────────┐    │              │  │ └─────────────┘ │    │      │\n│  │  │      CRDs       │    │              │  │                 │    │      │\n│  │  │   (API Server   │    │              │  │ ┌─────────────┐ │    │      │\n│  │  │   Extensions)   │    │              │  │ │ cainjector  │ │    │      │\n│  │  │                 │    │              │  │ └─────────────┘ │    │      │\n│  │  │ • certificates  │    │              │  │                 │    │      │\n│  │  │ • issuers       │    │              │  │ ┌─────────────┐ │    │      │\n│  │  │ • clusterissuers│    │              │  │ │   webhook   │ │    │      │\n│  │  │ • cert-requests │    │              │  │ └─────────────┘ │    │      │\n│  │  │ • challenges    │    │              │  └─────────────────┘    │      │\n│  │  │ • orders        │    │              └─────────────────────────┘      │\n│  │  └─────────────────┘    │                                               │\n│  └─────────────────────────┘                                               │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │   CRD Layer         │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │CustomResource │  │\n                          │  │ Definitions   │  │\n                          │  │               │  │\n                          │  │ 1. certificates│  │\n                          │  │ 2. issuers    │  │\n                          │  │ 3. clusteriss │  │\n                          │  │ 4. certrequests│  │\n                          │  │ 5. challenges │  │\n                          │  │ 6. orders     │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │Custom Resource│  │\n                          │  │   Instances   │  │\n                          │  │               │  │\n                          │  │ • example-cert│  │\n                          │  │ • example-iss │  │\n                          │  │ • secrets     │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              CRD Workflow:\n                    ┌─────────────────────────────────┐\n                    │    1. CRD Installation          │\n                    │   (kubectl apply cert-manager) │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. API Server Extension      │\n                    │   New resource types available │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. Controller Deployment     │\n                    │   cert-manager pods running    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Custom Resource Creation  │\n                    │   Certificate, Issuer objects  │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    5. Controller Processing     │\n                    │   Reconciliation loops         │\n                    └─────────────────────────────────┘\n\n                              kubectl explain Flow:\n                    ┌─────────────────────────────────┐\n                    │    kubectl explain command      │\n                    │   certificate.spec.subject     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    API Server Query             │\n                    │   OpenAPI schema lookup        │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    CRD Schema Retrieval         │\n                    │   Field definitions & types    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    Documentation Display        │\n                    │   Field descriptions & help    │\n                    └─────────────────────────────────┘\n\n                              File Output Structure:\n                    ┌─────────────────────────────────┐\n                    │       ~/resources.yaml          │\n                    │                                 │\n                    │ certificaterequests.cert-...    │\n                    │ certificates.cert-manager.io    │\n                    │ challenges.acme.cert-...        │\n                    │ clusterissuers.cert-...         │\n                    │ issuers.cert-manager.io         │\n                    │ orders.acme.cert-manager.io     │\n                    │                                 │\n                    │ (Default kubectl output format) │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │       ~/subject.yaml            │\n                    │                                 │\n                    │ GROUP: cert-manager.io          │\n                    │ KIND: Certificate               │\n                    │ VERSION: v1                     │\n                    │                                 │\n                    │ FIELD: subject <Object>         │\n                    │                                 │\n                    │ DESCRIPTION: X509 subject...    │\n                    │                                 │\n                    │ FIELDS:                         │\n                    │   countries <[]string>          │\n                    │   localities <[]string>         │\n                    │   organizations <[]string>      │\n                    │   ...                           │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. CRD COMPONENTS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │    CRD      │    │   Custom    │    │ Controller  │\n   │(Definition) │    │ Resources   │    │(Logic)      │\n   │Schema/API   │    │(Instances)  │    │Reconcile    │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. KUBECTL EXPLAIN (CRITICAL):\n   • Works with both native and custom resources\n   • Shows field types, descriptions, and structure\n   • Supports nested field paths (e.g., spec.subject.countries)\n   • Uses OpenAPI schema from CRD definitions\n\n3. OUTPUT FORMATS:\n   • Default: Human-readable tabular format\n   • YAML: -o yaml\n   • JSON: -o json\n   • Wide: -o wide\n   • Custom: -o custom-columns\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 5-8 minutes maximum\n   • Use grep to filter CRDs efficiently\n   • Redirect output with > for file creation\n   • Verify file contents with cat\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Using wrong output format (question specifies default)\n   ❌ Not filtering for cert-manager CRDs specifically\n   ❌ Incorrect file paths (must be in home directory ~/)\n   ❌ Not verifying file creation and contents\n   ❌ Wrong kubectl explain syntax\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get pods -n cert-manager (verify deployment)\n   ✅ kubectl get crd | grep cert-manager (check CRDs exist)\n   ✅ cat ~/resources.yaml (verify file contents)\n   ✅ cat ~/subject.yaml (verify documentation extracted)\n   ✅ ls -la ~/ (check file permissions and sizes)\n\n4. EFFICIENT WORKFLOW:\n   • Step 1: Verify cert-manager is running\n   • Step 2: List and filter CRDs\n   • Step 3: Save to file with redirection\n   • Step 4: Use kubectl explain for documentation\n   • Step 5: Save documentation to file\n   • Step 6: Verify both files created correctly\n\n📚 TROUBLESHOOTING & MAINTENANCE DOMAIN (15% of CKA):\n\n• CRDs: Custom Resource Definitions extend Kubernetes API\n• Custom Resources: Instances of CRDs with custom logic\n• Controllers: Implement business logic for custom resources\n• OpenAPI Schema: Defines structure and validation for CRDs\n• kubectl explain: Documentation and field exploration tool\n\n🔧 TROUBLESHOOTING TIPS:\n\nCRDs not showing?\n→ Check: CRD installation, API server connectivity, RBAC permissions\n\nkubectl explain not working?\n→ Check: CRD exists, correct resource name, API server availability\n\nFile not created?\n→ Check: Directory permissions, disk space, correct path syntax\n\nEmpty or wrong file contents?\n→ Check: Command syntax, output redirection, grep filters\n\n💡 EXAM DAY REMINDERS:\n\n1. Use kubectl's default output format unless specified otherwise\n2. Always verify file creation with cat or ls commands\n3. Use grep to filter specific CRDs from the full list\n4. kubectl explain works with nested field paths\n5. Files must be created in home directory (~/)\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get crd (list all CRDs)\n• kubectl describe crd NAME (detailed CRD information)\n• kubectl explain RESOURCE.FIELD (field documentation)\n• kubectl api-resources (list all available resources)\n• kubectl get events (check for CRD-related events)\n\n📋 KUBECTL EXPLAIN SYNTAX:\n\n```bash\n# Basic resource explanation\nkubectl explain RESOURCE\n\n# Specific field explanation\nkubectl explain RESOURCE.FIELD\n\n# Nested field explanation\nkubectl explain RESOURCE.FIELD.SUBFIELD\n\n# Examples:\nkubectl explain certificate\nkubectl explain certificate.spec\nkubectl explain certificate.spec.subject\nkubectl explain certificate.spec.subject.countries\n```\n\n🎯 FILE REDIRECTION PATTERNS:\n\n```bash\n# Save command output to file\nkubectl COMMAND > ~/filename.yaml\n\n# Append to existing file\nkubectl COMMAND >> ~/filename.yaml\n\n# Save with error handling\nkubectl COMMAND > ~/filename.yaml 2>&1\n\n# Verify file creation\ncat ~/filename.yaml\nls -la ~/filename.yaml\nwc -l ~/filename.yaml\n```\n\n🔧 CRD EXPLORATION COMMANDS:\n\n```bash\n# List all CRDs\nkubectl get crd\n\n# Filter CRDs by name pattern\nkubectl get crd | grep PATTERN\n\n# Get CRD details\nkubectl describe crd CRD-NAME\n\n# List custom resources of a type\nkubectl get CUSTOM-RESOURCE-TYPE\n\n# Explain custom resource fields\nkubectl explain CUSTOM-RESOURCE-TYPE.spec\n```\n\n🎯 CERT-MANAGER SPECIFIC:\n\n**Main CRDs**:\n- certificates.cert-manager.io: SSL/TLS certificates\n- issuers.cert-manager.io: Certificate issuers (namespace-scoped)\n- clusterissuers.cert-manager.io: Certificate issuers (cluster-scoped)\n- certificaterequests.cert-manager.io: Certificate signing requests\n- challenges.acme.cert-manager.io: ACME challenges\n- orders.acme.cert-manager.io: ACME orders\n\n**Common Fields**:\n- spec.subject: X.509 certificate subject attributes\n- spec.issuerRef: Reference to certificate issuer\n- spec.dnsNames: Subject Alternative Names\n- spec.secretName: Target secret for certificate storage"
  },
  {
    "id": "Q11",
    "title": "Gateway API",
    "category": "Services & Networking",
    "difficulty": "Hard",
    "task": "Task:\nMigrate an existing web application from Ingress to Gateway API. You must maintain HTTPS access.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) Web application already deployed\nkubectl get deployment,service web  # web app running\n\n# 2) TLS certificate and secret already created\nkubectl get secret web-cert  # TLS secret available\n\n# 3) Gateway API CRDs already installed\nkubectl get crd | grep gateway  # Gateway API resources available\n\n# 4) Existing Ingress resource ready for migration\nkubectl get ingress web  # Ingress with HTTPS and routing configuration\n\n# 5) nginx GatewayClass available\nkubectl get gatewayclass nginx  # GatewayClass ready for use",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Examine existing Ingress resource to understand current configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get ingress web -o yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web\n  namespace: default\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: gateway.web.k8s.local\n    http:\n      paths:\n      - backend:\n          service:\n            name: web\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - gateway.web.k8s.local\n    secretName: web-cert\n\n# Key information extracted:\n# - Hostname: gateway.web.k8s.local\n# - TLS secret: web-cert\n# - Backend service: web (port 80)\n# - Path: / (PathPrefix)\n# - IngressClass: nginx\n\n# 3) Verify GatewayClass is available\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get gatewayclass\nNAME    CONTROLLER                           ACCEPTED   AGE\nnginx   nginx.org/nginx-gateway-controller   Unknown    5m\n\n# 4) Create Gateway resource\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim gateway.yaml\n\n# Press 'i' to enter insert mode, then create:\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: web-local-gateway\nspec:\n  gatewayClassName: nginx\n  listeners:\n  - name: https\n    protocol: HTTPS\n    port: 443\n    hostname: gateway.web.k8s.local\n    tls:\n      mode: Terminate\n      certificateRefs:\n      - name: web-cert\n\n# Press Esc, then :wq to save and exit\n\n# 5) Apply the Gateway\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f gateway.yaml\ngateway.gateway.networking.k8s.io/web-local-gateway created\n\n# 6) Verify Gateway creation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get gateway web-local-gateway\nNAME                CLASS   ADDRESS   PROGRAMMED   AGE\nweb-local-gateway   nginx             Unknown      10s\n\n# Note: PROGRAMMED status may show \"Unknown\" in simulation environment\n# In real exam with proper Gateway controller, it should show \"True\"\n\n# 7) Create HTTPRoute resource\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim httproute.yaml\n\n# Press 'i' to enter insert mode, then create:\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: web-route\nspec:\n  parentRefs:\n  - name: web-local-gateway\n  hostnames:\n  - \"gateway.web.k8s.local\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: web\n      port: 80\n\n# Press Esc, then :wq to save and exit\n\n# 8) Apply the HTTPRoute\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f httproute.yaml\nhttproute.gateway.networking.k8s.io/web-route created\n\n# 9) Verify HTTPRoute creation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get httproute web-route\nNAME        HOSTNAMES                   AGE\nweb-route   [\"gateway.web.k8s.local\"]   15s\n\n# 10) Test Gateway API configuration (if controller is available)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl -Lk https://gateway.web.k8s.local:31443\n# Note: This test may not work in simulation environment without proper Gateway controller\n# In real exam, this should return the nginx welcome page\n\n# 11) Delete the existing Ingress resource\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl delete ingress web\ningress.networking.k8s.io \"web\" deleted\n\n# 12) Verify Ingress deletion\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get ingress\nNo resources found in default namespace.\n\n# 13) Final verification of Gateway API migration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get gateway,httproute\nNAME                                                  CLASS   ADDRESS   PROGRAMMED   AGE\ngateway.gateway.networking.k8s.io/web-local-gateway   nginx             Unknown      2m\n\nNAME                                            HOSTNAMES                   AGE\nhttproute.gateway.networking.k8s.io/web-route   [\"gateway.web.k8s.local\"]   90s\n\n# 14) Verify backend service is still available\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get service web\nNAME   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nweb    ClusterIP   10.103.107.208   <none>        80/TCP    10m\n\n# 15) Check Gateway and HTTPRoute status (optional)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe gateway web-local-gateway\nName:         web-local-gateway\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  gateway.networking.k8s.io/v1\nKind:         Gateway\nSpec:\n  Gateway Class Name:  nginx\n  Listeners:\n    Hostname:  gateway.web.k8s.local\n    Name:      https\n    Port:      443\n    Protocol:  HTTPS\n    Tls:\n      Certificate Refs:\n        Name:  web-cert\n      Mode:    Terminate\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe httproute web-route\nName:         web-route\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  gateway.networking.k8s.io/v1\nKind:         HTTPRoute\nSpec:\n  Hostnames:\n    gateway.web.k8s.local\n  Parent Refs:\n    Name:  web-local-gateway\n  Rules:\n    Backend Refs:\n      Name:  web\n      Port:  80\n    Matches:\n      Path:\n        Type:   PathPrefix\n        Value:  /\n\n# 16) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ Gateway \"web-local-gateway\" created with hostname \"gateway.web.k8s.local\"\n# ✅ HTTPS listener configured with TLS termination using web-cert secret\n# ✅ HTTPRoute \"web-route\" created with correct hostname and backend references\n# ✅ Routing rules maintained from original Ingress (PathPrefix / → web:80)\n# ✅ TLS configuration preserved from original Ingress\n# ✅ Original Ingress resource \"web\" successfully deleted\n# ✅ Migration from Ingress to Gateway API completed while maintaining HTTPS access",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         Gateway API & Ingress Migration                    │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │   Web Pod       │    │      │\n│  │  │  - API Server   │    │              │  │                 │    │      │\n│  │  │  - Gateway API  │    │              │  │ ┌─────────────┐ │    │      │\n│  │  │    CRDs         │    │              │  │ │   nginx     │ │    │      │\n│  │  │  - kubectl CLI  │    │              │  │ │   :80       │ │    │      │\n│  │  └─────────────────┘    │              │  │ └─────────────┘ │    │      │\n│  │                         │              │  └─────────────────┘    │      │\n│  │  ┌─────────────────┐    │              │                         │      │\n│  │  │   TLS Secret    │    │              │  ┌─────────────────┐    │      │\n│  │  │   web-cert      │    │              │  │   Service       │    │      │\n│  │  │                 │    │              │  │     web         │    │      │\n│  │  │ • tls.crt       │    │              │  │                 │    │      │\n│  │  │ • tls.key       │    │              │  │ ClusterIP:80    │    │      │\n│  │  └─────────────────┘    │              │  └─────────────────┘    │      │\n│  └─────────────────────────┘              └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │ Gateway API Layer   │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │ GatewayClass  │  │\n                          │  │    nginx      │  │\n                          │  │               │  │\n                          │  │ Controller:   │  │\n                          │  │nginx.org/nginx│  │\n                          │  │-gateway-ctrl  │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │   Gateway     │  │\n                          │  │web-local-gate │  │\n                          │  │               │  │\n                          │  │ Listener:     │  │\n                          │  │ - HTTPS:443   │  │\n                          │  │ - TLS Term    │  │\n                          │  │ - web-cert    │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │  HTTPRoute    │  │\n                          │  │  web-route    │  │\n                          │  │               │  │\n                          │  │ Host: gateway │  │\n                          │  │ .web.k8s.local│  │\n                          │  │ Path: /       │  │\n                          │  │ Backend: web  │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Migration Flow:\n                    ┌─────────────────────────────────┐\n                    │    1. Analyze Ingress           │\n                    │   Extract configuration         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. Create Gateway            │\n                    │   HTTPS listener + TLS config  │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. Create HTTPRoute          │\n                    │   Routing rules + backends     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Test Configuration        │\n                    │   Verify HTTPS access          │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    5. Delete Ingress            │\n                    │   Complete migration           │\n                    └─────────────────────────────────┘\n\n                              Before vs After:\n                    ┌─────────────────────────────────┐\n                    │         BEFORE (Ingress)       │\n                    │                                 │\n                    │ ┌─────────────────────────────┐ │\n                    │ │        Ingress              │ │\n                    │ │                             │ │\n                    │ │ Host: gateway.web.k8s.local │ │\n                    │ │ TLS: web-cert               │ │\n                    │ │ Path: / → web:80            │ │\n                    │ │ Class: nginx                │ │\n                    │ └─────────────────────────────┘ │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         AFTER (Gateway API)    │\n                    │                                 │\n                    │ ┌─────────────────────────────┐ │\n                    │ │       Gateway               │ │\n                    │ │                             │ │\n                    │ │ Class: nginx                │ │\n                    │ │ Listener: HTTPS:443         │ │\n                    │ │ TLS: Terminate (web-cert)   │ │\n                    │ │ Host: gateway.web.k8s.local │ │\n                    │ └─────────────────────────────┘ │\n                    │              +                  │\n                    │ ┌─────────────────────────────┐ │\n                    │ │      HTTPRoute              │ │\n                    │ │                             │ │\n                    │ │ Parent: web-local-gateway   │ │\n                    │ │ Host: gateway.web.k8s.local │ │\n                    │ │ Path: / → web:80            │ │\n                    │ └─────────────────────────────┘ │\n                    └─────────────────────────────────┘\n\n                              Traffic Flow:\n                    ┌─────────────────────────────────┐\n                    │         Client Request          │\n                    │   https://gateway.web.k8s.local │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         Gateway                 │\n                    │   TLS Termination               │\n                    │   Certificate: web-cert         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         HTTPRoute               │\n                    │   Host + Path Matching          │\n                    │   Route Selection               │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         Backend Service         │\n                    │   web:80 → nginx pod            │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. GATEWAY API COMPONENTS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │GatewayClass │    │   Gateway   │    │ HTTPRoute   │\n   │(Controller) │    │(Listeners)  │    │(Routing)    │\n   │Infrastructure│    │TLS/Protocol │    │Rules/Backends│\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. MIGRATION MAPPING (CRITICAL):\n   • Ingress.spec.ingressClassName → Gateway.spec.gatewayClassName\n   • Ingress.spec.tls → Gateway.listeners[].tls\n   • Ingress.spec.rules[].host → HTTPRoute.spec.hostnames[]\n   • Ingress.spec.rules[].http.paths → HTTPRoute.spec.rules[].matches\n   • Ingress backend → HTTPRoute backendRefs\n\n3. TLS CONFIGURATION:\n   • mode: Terminate (TLS termination at gateway)\n   • certificateRefs: Reference to TLS secret\n   • hostname: Must match certificate CN/SAN\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 12-18 minutes maximum\n   • Analyze existing Ingress first\n   • Create Gateway and HTTPRoute systematically\n   • Test configuration before deleting Ingress\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Wrong Gateway/HTTPRoute names (question specifies exact names)\n   ❌ Missing hostname in Gateway listener\n   ❌ Incorrect TLS certificate reference\n   ❌ Wrong parent reference in HTTPRoute\n   ❌ Deleting Ingress before verifying Gateway works\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get gateway (check PROGRAMMED status)\n   ✅ kubectl get httproute (verify hostnames)\n   ✅ kubectl describe gateway (check listener configuration)\n   ✅ kubectl describe httproute (verify backend references)\n   ✅ Test HTTPS access (if controller available)\n   ✅ kubectl get ingress (verify deletion)\n\n4. MIGRATION WORKFLOW:\n   • Step 1: kubectl get ingress NAME -o yaml (analyze configuration)\n   • Step 2: Create Gateway with matching TLS and hostname\n   • Step 3: Create HTTPRoute with matching rules and backends\n   • Step 4: Test configuration (curl or describe)\n   • Step 5: Delete original Ingress\n   • Step 6: Final verification\n\n📚 SERVICES & NETWORKING DOMAIN (20% of CKA):\n\n• Gateway API: Next-generation ingress and traffic management\n• Ingress: Traditional HTTP/HTTPS routing (being superseded)\n• TLS Termination: SSL/TLS certificate handling at gateway\n• Traffic Routing: Host-based and path-based routing rules\n• Service Mesh Integration: Advanced traffic management capabilities\n\n🔧 TROUBLESHOOTING TIPS:\n\nGateway not PROGRAMMED?\n→ Check: GatewayClass exists, controller running, valid configuration\n\nHTTPRoute not working?\n→ Check: Parent reference correct, hostnames match, backend service exists\n\nTLS issues?\n→ Check: Certificate secret exists, hostname matches CN/SAN, TLS mode correct\n\nBackend connection failed?\n→ Check: Service name/port correct, pods running, network policies\n\n💡 EXAM DAY REMINDERS:\n\n1. Always analyze existing Ingress configuration first\n2. Use exact names specified in the question\n3. Maintain all TLS and routing configurations\n4. Test before deleting original Ingress\n5. Gateway API uses different field names than Ingress\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get ingress NAME -o yaml (analyze source)\n• kubectl describe gateway NAME (check listeners)\n• kubectl describe httproute NAME (check rules)\n• kubectl get events (check controller events)\n• kubectl logs -n CONTROLLER-NS (check controller logs)\n\n📋 GATEWAY YAML TEMPLATE:\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: GATEWAY-NAME\nspec:\n  gatewayClassName: GATEWAY-CLASS\n  listeners:\n  - name: https\n    protocol: HTTPS\n    port: 443\n    hostname: HOSTNAME\n    tls:\n      mode: Terminate\n      certificateRefs:\n      - name: TLS-SECRET-NAME\n```\n\n📋 HTTPROUTE YAML TEMPLATE:\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: ROUTE-NAME\nspec:\n  parentRefs:\n  - name: GATEWAY-NAME\n  hostnames:\n  - \"HOSTNAME\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: PATH\n    backendRefs:\n    - name: SERVICE-NAME\n      port: SERVICE-PORT\n```\n\n🎯 INGRESS TO GATEWAY MAPPING:\n\n**Ingress Fields → Gateway API Fields**:\n```yaml\n# Ingress\nspec:\n  ingressClassName: nginx          → gatewayClassName: nginx\n  tls:\n  - hosts: [host]                  → listeners[].hostname: host\n    secretName: secret             → listeners[].tls.certificateRefs[].name: secret\n  rules:\n  - host: hostname                 → hostnames: [hostname]\n    http:\n      paths:\n      - path: /path                → rules[].matches[].path.value: /path\n        pathType: Prefix           → rules[].matches[].path.type: PathPrefix\n        backend:\n          service:\n            name: svc              → backendRefs[].name: svc\n            port:\n              number: 80           → backendRefs[].port: 80\n```\n\n🔧 GATEWAY API ADVANTAGES:\n\n**Over Ingress**:\n- More expressive routing rules\n- Better multi-protocol support\n- Cleaner separation of concerns\n- Enhanced traffic management\n- Future-proof architecture\n\n**Resource Separation**:\n- GatewayClass: Infrastructure/controller config\n- Gateway: Listener and TLS configuration\n- HTTPRoute: Application routing rules\n- Policy attachment: Cross-cutting concerns"
  },
  {
    "id": "Q12",
    "title": "Sidecar Container",
    "category": "Workloads & Scheduling",
    "difficulty": "Medium",
    "task": "To integrate traditional applications into Kubernetes' logging architecture (such as kubectl logs), the common practice is to add a sidecar container for log streaming.",
    "environment": "",
    "preparation": "# 1) Create deployment resource\n# Import busybox stable image on worker node\nroot@worker-node:~# ctr -n=k8s.io images import busybox_stable.tar\n\ncat <<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sync-leverager\n  labels:\n    app: sync-leverager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sync-leverager\n  template:\n    metadata:\n      labels:\n        app: sync-leverager\n    spec:\n      containers:\n      - name: sync-leverager\n        image: nginx:1.25\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"-c\", \"while true; do echo \\\"$(date) INFO log line\\\" >> /var/log/sync-leverager.log; sleep 5; done\"]\nEOF",
    "steps": "# 1) Connect to the specified node (in real exam)\n[candidate@base ~]$ ssh cka000012\n[candidate@cka000012~]$\n\n# Note: In simulation environment, work directly from candidate@base:~$ terminal\n# In real exam, SSH to the specified node as required\n\n# 2) Export the existing deployment to YAML file\n[candidate@cka000012 ~]$ kubectl get deployment sync-leverager -o yaml > sidecar.yaml\n\n# 3) Edit the YAML file\n[candidate@cka000012 ~]$ vim sidecar.yaml\n\n# Note: Use :set paste before pressing i to insert to prevent formatting issues\n# The following is the modified YAML (insertion position and indentation adjusted according to actual exam requirements)\n\n# Complete YAML should look like this:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sync-leverager\n  labels:\n    app: sync-leverager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sync-leverager\n  template:\n    metadata:\n      labels:\n        app: sync-leverager\n    spec:\n      volumes:  # Add this section\n      - name: varlog\n        emptyDir: {}\n      containers:\n      - name: sync-leverager\n        image: nginx:1.25\n        command: [\"/bin/sh\", \"-c\", \"while true; do echo \\\"$(date) INFO log line\\\" >> /var/log/sync-leverager.log; sleep 5; done\"]\n        volumeMounts:  # Add this section to existing container\n        - name: varlog\n          mountPath: /var/log\n      - name: sidecar  # Add this entire sidecar container\n        image: busybox:stable\n        imagePullPolicy: IfNotPresent\n        args: [\"/bin/sh\", \"-c\", \"tail -n+1 -f /var/log/sync-leverager.log\"]\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n\n# Key modifications:\n# 1. Add volumes section with emptyDir volume named \"varlog\"\n# 2. Add volumeMounts to existing sync-leverager container\n# 3. Add new sidecar container with specified image and command\n# 4. Add volumeMounts to sidecar container\n\n# Apply the modified deployment\n[candidate@cka000012 ~]$ kubectl apply -f sidecar.yaml\n\n# 4) Check deployment status\n[candidate@cka000012 ~]$ kubectl get deployment sync-leverager\n\n# Expected output:\n# NAME             READY   UP-TO-DATE   AVAILABLE   AGE\n# sync-leverager   1/1     1            1           10m\n\n# 5) Check if pod is in running state and contains two containers\n[candidate@cka000012 ~]$ kubectl get pod | grep sync-leverager\n\n# Expected output:\n# sync-leverager-74c79d54cf-xr9cz   2/2     Running   0          2m\n\n# The \"2/2\" indicates both containers are running successfully\n\n# 6) View sidecar container logs\n[candidate@cka000012 ~]$ kubectl logs sync-leverager-74c79d54cf-xr9cz -c sidecar\n\n# Expected output (similar content indicates successful sidecar container configuration):\n# Sun Mar 23 05:15:22 UTC 2025 INFO log line\n# Sun Mar 23 05:15:27 UTC 2025 INFO log line\n# Sun Mar 23 05:15:32 UTC 2025 INFO log line\n# Sun Mar 23 05:15:37 UTC 2025 INFO log line\n# Sun Mar 23 05:15:42 UTC 2025 INFO log line\n\n# 7) Additional verification commands\n# Check pod details\n[candidate@cka000012 ~]$ kubectl describe pod <pod-name>\n\n# Verify both containers are present\n[candidate@cka000012 ~]$ kubectl get pod <pod-name> -o jsonpath='{.spec.containers[*].name}'\n\n# Should output: sync-leverager sidecar\n\n# Check volume mounts\n[candidate@cka000012 ~]$ kubectl describe pod <pod-name> | grep -A 10 \"Mounts:\"\n\n# Test log file sharing between containers\n[candidate@cka000012 ~]$ kubectl exec <pod-name> -c sync-leverager -- ls -la /var/log/\n[candidate@cka000012 ~]$ kubectl exec <pod-name> -c sidecar -- ls -la /var/log/\n\n# Both should show the sync-leverager.log file\n\n# 8) Exit current node and return to base node (in real exam)\n[candidate@cka000012~]$ exit\n[candidate@base~]$",
    "diagram": "",
    "tips": ""
  },
  {
    "id": "Q13",
    "title": "Calico CNI",
    "category": "Services & Networking",
    "difficulty": "Hard",
    "task": "Documentation Links:\nFlannel Manifest: https://github.com/flannel-io/flannel/releases/download/v0.26.1/kube-flannel.yml\nCalico Manifest: https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml",
    "environment": "",
    "preparation": "# Note: This question requires choosing between Flannel and Calico\n# Since Flannel does not support NetworkPolicy enforcement, Calico must be chosen\n\n# Import Calico images on worker node (simulation environment only)\nroot@worker-node:~# ctr -n=k8s.io images import calico-operator.tar\nroot@worker-node:~# ctr -n=k8s.io images import calico_oper.tar",
    "steps": "# Note: In simulation environment, take a snapshot before this exercise as it may affect the k8s cluster\n# Restore from snapshot after completing this question to continue with other exercises\n# In real exam, no snapshot is needed, proceed directly\n\n# 1) Connect to the specified node as required by the question (mandatory, zero points if not done)\n[candidate@base]$ ssh cka000013\n[candidate@cka000013~]$\n\n# 2) Download and create Calico tigera-operator.yaml file\n# Choose Calico v3.27.0 (Reason: Flannel does not support NetworkPolicy, only Calico can be chosen)\n\n[candidate@cka000013~]$ wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\n\n# Note: In simulation environment, direct download may not work, tigera-operator.yaml file is provided in course materials\n\n# Deploy tigera-operator (must use create, not apply)\n[candidate@cka000013~]$ kubectl create -f tigera-operator.yaml\n\n# Note: Must use create, not apply, as using apply directly may cause CRD conflicts\n# If simulation environment shows \"already exists\", ignore it as some resources are already deployed\n\n# 3) Check Pod CIDR (needed for custom-resources.yaml)\n[candidate@cka000013~]$ kubectl cluster-info dump | grep -i cluster-cidr\n\n# Expected output:\n# \"clusterCIDR\": \"10.244.0.0/16\",\n# Remember this address: 10.244.0.0/16\n\n# 4) Download Calico custom resource configuration custom-resources.yaml\n[candidate@cka000013~]$ wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Note: This URL is not provided in the exam, but can be derived from the tigera-operator.yaml download address\n# by remembering the word \"custom-resources.yaml\"\n# In simulation environment, custom-resources.yaml file is provided in course materials\n\n# 5) Edit custom-resources.yaml file to modify the cidr field\n[candidate@cka000013~]$ vim custom-resources.yaml\n\n# Use :set paste before pressing i to insert to prevent formatting issues\n# Modify the following content (key point is to fill cidr with 10.244.0.0/16 found in previous step):\n\napiVersion: operator.tigera.io/v1\nkind: Installation\nmetadata:\n  name: default\nspec:\n  calicoNetwork:\n    ipPools:\n    - blockSize: 26\n      cidr: 10.244.0.0/16  # Write the pod network segment found in the previous step\n      encapsulation: VXLANCrossSubnet\n      natOutgoing: Enabled\n      nodeSelector: all()\n\n# 6) Create Calico custom resources\n[candidate@cka000013~]$ kubectl create -f custom-resources.yaml\n\n# Note: If simulation environment shows \"already exists\", it means it was already created, ignore it\n\n# 7) Check Calico component running status (all should be Running after about 2 minutes)\n[candidate@cka000013~]$ kubectl -n calico-system get pod\n\n# Expected output:\n# NAME                                      READY   STATUS    RESTARTS   AGE\n# calico-kube-controllers-xxxx              1/1     Running   0          2m\n# calico-node-xxxx                          1/1     Running   0          2m\n# calico-typha-xxxx                         1/1     Running   0          2m\n\n# Note: In simulation environment, you might see mixed status (some Running, some Error)\n# This can be ignored as it's due to Calico being installed twice in simulation\n# In real exam, ensure all pods are Running\n\n# 8) Additional verification commands\n# Check Calico installation status\n[candidate@cka000013~]$ kubectl get installation default -o yaml\n\n# Verify network policies can be created (test NetworkPolicy support)\n[candidate@cka000013~]$ kubectl get networkpolicies --all-namespaces\n\n# Check node status\n[candidate@cka000013~]$ kubectl get nodes\n\n# Verify pod-to-pod communication\n[candidate@cka000013~]$ kubectl run test-pod --image=busybox --rm -it -- /bin/sh\n\n# 9) Exit current node and return to base node (in real exam)\n[candidate@cka000013~]$ exit\n[candidate@base~]$\n\n# Important Notes:\n# - Calico is chosen over Flannel because NetworkPolicy support is required\n# - The tigera-operator must be installed first, followed by custom resources\n# - The Pod CIDR must match the cluster's existing configuration\n# - All Calico system pods must be in Running state for successful installation",
    "diagram": "",
    "tips": ""
  },
  {
    "id": "Q14",
    "title": "ArgoCD Installation",
    "category": "Troubleshooting",
    "difficulty": "Medium",
    "task": "Quick Reference: Argo Helm Charts Documentation",
    "environment": "",
    "preparation": "# Install Helm\nsudo su -\n# Enter password: 111111\ntar zxvf helm-v3.17.1-linux-amd64.tar.gz\nmv linux-amd64/helm /usr/bin/\nhelm version\n\n# Create argocd namespace\nkubectl create ns argocd\n\n# Import Argo CD images on worker node\nroot@worker-node:~# ctr -n=k8s.io images import argo.tar",
    "steps": "# 1) Connect to the specified node (in real exam)\n[candidate@base] $ ssh cka000014\n[candidate@cka000014~]$\n\n# Note: In simulation environment, work directly from candidate@base:~$ terminal\n# In real exam, SSH to the specified node as required\n\n# 2) Add official Argo CD Helm repository and update\n[candidate@cka000014~]$ helm repo add argo https://argoproj.github.io/argo-helm\n\n# Update repository (important step)\n[candidate@cka000014~]$ helm repo update\n\n# 3) Search for Argo CD Chart to verify successful addition (optional)\n[candidate@cka000014~]$ helm search repo argo | grep argo-cd\n\n# Expected output:\n# argo/argo-cd    7.8.13    A Helm chart for Argo CD, a declarative, GitOps continuous...\n\n# 4) Generate Argo CD Helm template (required)\n# Version must be 5.5.22 as specified in the question, not the latest version from search\n[candidate@cka000014~]$ helm template argocd argo/argo-cd --version 5.5.22 --namespace argocd --set crds.install=false > ~/argo-helm.yaml\n\n# Command breakdown:\n# - helm template: Generate template without installing\n# - argocd: Release name\n# - argo/argo-cd: Chart name from repository\n# - --version 5.5.22: Specific version as required\n# - --namespace argocd: Target namespace\n# - --set crds.install=false: Do not install CRDs\n# - > ~/argo-helm.yaml: Save output to file\n\n# 5) Install Argo CD using Helm\n# Release name: argocd, version: 5.5.22, namespace: argocd, no CRDs installation\n[candidate@cka000014~]$ helm install argocd argo/argo-cd --version 5.5.22 --namespace argocd --set crds.install=false\n\n# Command breakdown:\n# - helm install: Install the chart\n# - argocd: Release name\n# - argo/argo-cd: Chart name from repository\n# - --version 5.5.22: Specific version as required\n# - --namespace argocd: Target namespace\n# - --set crds.install=false: Do not install CRDs\n\n# 6) Verify Argo CD installation\n# Check if pods exist in argocd namespace (they don't need to all be Running immediately)\n[candidate@cka000014~]$ kubectl -n argocd get pods\n\n# Expected output (pods should be created, Running status will come shortly):\n# NAME                                 READY   STATUS    RESTARTS   AGE\n# argocd-application-controller-0      1/1     Running   0          2m\n# argocd-dex-server-xxx                1/1     Running   0          2m\n# argocd-redis-xxx                     1/1     Running   0          2m\n# argocd-repo-server-xxx               1/1     Running   0          2m\n# argocd-server-xxx                    1/1     Running   0          2m\n\n# Additional verification commands (optional)\n[candidate@cka000014~]$ helm list -n argocd\n[candidate@cka000014~]$ kubectl -n argocd get all\n\n# 7) Exit current node and return to base node (in real exam)\n[candidate@cka000014~]$ exit\n[candidate@base~]$\n\n# Additional Notes:\n# - The template file ~/argo-helm.yaml contains all the Kubernetes manifests that would be applied\n# - CRDs are not installed because --set crds.install=false is used\n# - In real exam environment, pods will typically reach Running state quickly\n# - The exact pod names will vary due to random suffixes generated by Kubernetes",
    "diagram": "",
    "tips": ""
  },
  {
    "id": "Q15",
    "title": "ETCD Troubleshooting",
    "category": "Troubleshooting",
    "difficulty": "Hard",
    "task": "Context:\nA kubeadm-configured cluster has been migrated to a new machine. It needs configuration changes to run successfully.",
    "environment": "",
    "preparation": "# Note: This is a troubleshooting scenario - no specific environment preparation needed\n# The cluster is already broken and needs to be fixed",
    "steps": "# Note: Simulation environment doesn't need to simulate this - just understand the solution approach\n\n# 1) Connect to the specified node (in real exam)\n[candidate@base ~]$ ssh cka000015\n[candidate@cka000015~]$ sudo -i\n[candidate@cka000015~]#\n\n# 2) Fix kube-apiserver configuration (etcd address)\n# Edit the kube-apiserver static Pod manifest file\n[candidate@cka000015 ~]# vim /etc/kubernetes/manifests/kube-apiserver.yaml\n\n# Find the --etcd-servers= parameter and change it to:\n- --etcd-servers=https://127.0.0.1:2379\n\n# This points to the local etcd server\n# Save and exit with :wq\n\n# 3) Restart kubelet service\n[candidate@cka000015 ~]# systemctl daemon-reload\n[candidate@cka000015 ~]# systemctl restart kubelet\n\n# 4) Check node and Pod status (scheduler might still have issues at this point)\n[candidate@cka000015 ~]# kubectl get nodes\n\n# Expected output:\n# NAME       STATUS   ROLES    AGE   VERSION\n# master01   Ready    master   10m   v1.32.1\n\n[candidate@cka000015 ~]# kubectl -n kube-system get pod\n\n# Expected output might show scheduler in CrashLoopBackOff:\n# NAME                            READY   STATUS             RESTARTS   AGE\n# kube-scheduler-master01         0/1     CrashLoopBackOff   5          2m\n\n# 5) Fix kube-scheduler configuration\n# Edit the kube-scheduler static Pod manifest file\nvim /etc/kubernetes/manifests/kube-scheduler.yaml\n\n# Find the resources configuration section and change it to:\nresources:\n  requests:\n    cpu: 100m\n\n# Note: Change the cpu request under requests to 100m\n# Save and exit with :wq\n\n# 6) Wait for kubelet to automatically restart kube-scheduler\n# Static Pod configuration files are automatically reloaded by kubelet\n# Wait about 1 minute for the changes to take effect\n\n# 7) Check cluster status again (nodes Ready, all Pods Running)\n[candidate@cka000015 ~]# kubectl get nodes\n\n# Expected output:\n# NAME       STATUS   ROLES    AGE   VERSION\n# master01   Ready    master   10m   v1.32.1\n\n[candidate@cka000015 ~]# kubectl -n kube-system get pod\n\n# Expected output (all pods should be Running):\n# NAME                                  READY   STATUS    RESTARTS   AGE\n# kube-scheduler-master01               1/1     Running   0          3m\n# kube-apiserver-master01               1/1     Running   0          3m\n# kube-controller-manager-master01      1/1     Running   0          3m\n# coredns-7bdc4cb885-ctmxl             1/1     Running   0          5m\n# coredns-7bdc4cb885-fvqdl             1/1     Running   0          5m\n\n# 8) Additional verification commands\n# Check cluster info\n[candidate@cka000015 ~]# kubectl cluster-info\n\n# Check component status\n[candidate@cka000015 ~]# kubectl get componentstatuses\n\n# Verify etcd is accessible\n[candidate@cka000015 ~]# kubectl -n kube-system get pods -l component=etcd\n\n# 9) Exit current node and return to base node (in real exam)\n[candidate@cka000015~]# exit\n[candidate@cka000015~]$ exit\n[candidate@base~]$\n\n# Troubleshooting Tips:\n# - Always check /var/log/kubelet.log for kubelet errors\n# - Check /var/log/pods/ for specific pod logs\n# - Use kubectl describe pod <pod-name> -n kube-system for detailed pod information\n# - Static Pod manifests are in /etc/kubernetes/manifests/\n# - Common issues include wrong etcd endpoints, resource constraints, and certificate problems",
    "diagram": "",
    "tips": ""
  },
  {
    "id": "Q16",
    "title": "CRI-Dockerd Setup",
    "category": "Troubleshooting",
    "difficulty": "Medium",
    "task": "Context:\nYour task is to prepare a Linux system for Kubernetes. Docker has been installed, but you need to configure it for kubeadm.",
    "environment": "",
    "preparation": "# Note: This is a system preparation task, typically performed on a fresh node\n# The Debian package should be provided in the home directory",
    "steps": "# 1) Connect to the specified node (in real exam)\n[candidate@base ~]$ ssh cka000016\n[candidate@cka000016~]$\n\n# Note: In simulation environment, work directly from candidate@base:~$ terminal\n# In real exam, SSH to the specified node as required\n\n# 2) Install cri-dockerd\n[candidate@cka000016 ~]$ sudo dpkg -i ~/cri-dockerd_0.3.6.3-0.ubuntu-jammy_amd64.deb\n\n# Expected output:\n# Selecting previously unselected package cri-dockerd.\n# (Reading database ... 185547 files and directories currently installed.)\n# Preparing to unpack .../cri-dockerd_0.3.6.3-0.ubuntu-jammy_amd64.deb ...\n# Unpacking cri-dockerd (0.3.6.3-0.ubuntu-jammy) ...\n# Setting up cri-dockerd (0.3.6.3-0.ubuntu-jammy) ...\n\n# If there are dependency issues, you might need to run:\n# sudo apt-get install -f\n\n# 3) Enable and start cri-docker service\n[candidate@cka000016 ~]$ sudo systemctl enable cri-docker\n\n# Expected output:\n# Created symlink /etc/systemd/system/multi-user.target.wants/cri-docker.service → /lib/systemd/system/cri-docker.service.\n\n[candidate@cka000016 ~]$ sudo systemctl start cri-docker\n\n# Verify the service is running\n[candidate@cka000016 ~]$ sudo systemctl status cri-docker\n\n# Expected output should show \"active (running)\"\n\n# 4) Configure system parameters (must ensure they persist after reboot)\n# Load the br_netfilter module\n[candidate@cka000016 ~]$ sudo modprobe br_netfilter\n\n# Make the module load automatically on boot\n[candidate@cka000016 ~]$ echo 'br_netfilter' | sudo tee /etc/modules-load.d/k8s.conf\n\n# Edit /etc/sysctl.conf to add kernel parameters\n[candidate@cka000016 ~]$ sudo vim /etc/sysctl.conf\n\n# In vim, execute the following:\n# :set paste\n# Press i to enter insert mode\n# Add the following content to the end of the file:\n\n# Kubernetes required kernel parameters\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv6.conf.all.forwarding = 1\nnet.ipv4.ip_forward = 1\nnet.netfilter.nf_conntrack_max = 131072\n\n# Save and exit: type :wq\n\n# 5) Apply the configuration immediately\n[candidate@cka000016 ~]$ sudo sysctl -p\n\n# Expected output:\n# net.bridge.bridge-nf-call-iptables = 1\n# net.ipv6.conf.all.forwarding = 1\n# net.ipv4.ip_forward = 1\n# net.netfilter.nf_conntrack_max = 131072\n\n# 6) Verify the configuration is applied\n[candidate@cka000016 ~]$ sudo sysctl net.bridge.bridge-nf-call-iptables\n[candidate@cka000016 ~]$ sudo sysctl net.ipv6.conf.all.forwarding\n[candidate@cka000016 ~]$ sudo sysctl net.ipv4.ip_forward\n[candidate@cka000016 ~]$ sudo sysctl net.netfilter.nf_conntrack_max\n\n# All should return the values set above (1, 1, 1, 131072 respectively)\n\n# 7) Additional verification commands\n# Check if cri-dockerd socket is available\n[candidate@cka000016 ~]$ sudo systemctl status cri-docker.socket\n\n# Verify Docker is running\n[candidate@cka000016 ~]$ sudo systemctl status docker\n\n# Test cri-dockerd connectivity\n[candidate@cka000016 ~]$ sudo cri-dockerd --version\n\n# Check if the br_netfilter module is loaded\n[candidate@cka000016 ~]$ lsmod | grep br_netfilter\n\n# 8) Alternative method for persistent sysctl configuration\n# Instead of editing /etc/sysctl.conf, you can create a dedicated file:\n# sudo vim /etc/sysctl.d/99-kubernetes-cri.conf\n# Add the same parameters there\n\n# 9) Troubleshooting commands (if needed)\n# Check system logs for cri-dockerd\n[candidate@cka000016 ~]$ sudo journalctl -u cri-docker -f\n\n# Check if all required services are enabled\n[candidate@cka000016 ~]$ sudo systemctl is-enabled docker cri-docker\n\n# Verify network bridge functionality\n[candidate@cka000016 ~]$ sudo iptables -L\n\n# 10) Exit current node and return to base node (in real exam)\n[candidate@cka000016~]$ exit\n[candidate@base~]$\n\n# Important Notes:\n# - cri-dockerd acts as a bridge between Docker and Kubernetes CRI\n# - System parameters must persist across reboots for production use\n# - The br_netfilter module is essential for Kubernetes networking\n# - All services should be enabled to start automatically on boot",
    "diagram": "",
    "tips": ""
  },
  {
    "id": "Q2",
    "title": "Service (L4)",
    "category": "Services & Networking",
    "difficulty": "Easy",
    "task": "Task:\nReconfigure the existing front-end Deployment in the spline namespace to expose port 80/tcp of the existing nginx container.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Flannel",
    "preparation": "# 1) Namespace and deployment already exist\nkubectl get ns spline           # namespace exists\nkubectl get deployment -n spline front-end  # deployment exists\n\n# 2) nginx:1.25 image will be pulled from Docker Hub automatically",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Verify existing deployment\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n spline front-end\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nfront-end   1/1     1            1           2m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n spline\nNAME                         READY   STATUS    RESTARTS   AGE\nfront-end-7df596788d-xxxxx   1/1     Running   0          2m\n\n# 3) Edit deployment to add port configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl edit deployment front-end -n spline\n\n# In the editor, locate the containers section:\n#   containers:\n#   - image: nginx:1.25\n#     imagePullPolicy: IfNotPresent\n#     name: nginx\n\n# Add ports configuration after name: nginx (same indentation level):\n#     ports:\n#     - containerPort: 80\n#       protocol: TCP\n\n# Complete section should look like:\n#   containers:\n#   - image: nginx:1.25\n#     imagePullPolicy: IfNotPresent\n#     name: nginx\n#     ports:\n#     - containerPort: 80\n#       protocol: TCP\n\n# Save and exit (:wq in vim)\n# Expected output: deployment.apps/front-end edited\n\n# Alternative method using kubectl patch:\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl patch deployment front-end -n spline -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"ports\":[{\"containerPort\":80,\"protocol\":\"TCP\"}]}]}}}}'\n\n# 4) Create NodePort service\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl expose deployment front-end -n spline --type=NodePort --port=80 --target-port=80 --name=front-end-svc\nservice/front-end-svc exposed\n\n# Parameter explanation:\n# --type=NodePort: Creates NodePort service as required\n# --port=80: Service port (what clients connect to)\n# --target-port=80: Container port (where traffic is forwarded)\n# --name=front-end-svc: Service name as specified in question\n\n# 5) Verify service creation and get details\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get svc front-end-svc -n spline -o wide\nNAME            TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE   SELECTOR\nfront-end-svc   NodePort   10.98.94.33   <none>        80:31365/TCP   30s   app=front-end\n\n# Note: ClusterIP and NodePort will vary in actual deployment\n\n# 6) Verify service endpoints\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get endpoints front-end-svc -n spline\nNAME            ENDPOINTS       AGE\nfront-end-svc   10.244.1.5:80   35s\n\n# 7) Test service functionality\n# Test ClusterIP access (internal)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl 10.98.94.33:80\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n\n# Test NodePort access (external)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl localhost:31365\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n\n# 8) Additional verification commands\n# Check deployment has been updated with port configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment front-end -n spline -o jsonpath='{.spec.template.spec.containers[0].ports}'\n[{\"containerPort\":80,\"protocol\":\"TCP\"}]\n\n# Check all resources in namespace\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get all -n spline\nNAME                             READY   STATUS    RESTARTS   AGE\npod/front-end-6d498499bc-xxxxx   1/1     Running   0          5m\n\nNAME                    TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE\nservice/front-end-svc   NodePort   10.98.94.33   <none>        80:31365/TCP   2m\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/front-end   1/1     1            1           5m\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/front-end-6d498499bc   1         1         1       5m\n\n# 9) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ Deployment reconfigured to expose port 80/tcp\n# ✅ NodePort service \"front-end-svc\" created\n# ✅ Service exposes individual pods through NodePort\n# ✅ Both internal (ClusterIP) and external (NodePort) access working\n# ✅ Service properly targets deployment pods via selector",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         Services & Networking (L4)                         │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │   nginx Pod     │    │      │\n│  │  │  - API Server   │    │              │  │  (spline ns)    │    │      │\n│  │  │  - kube-proxy   │    │              │  │                 │    │      │\n│  │  └─────────────────┘    │              │  │ ┌─────────────┐ │    │      │\n│  │                         │              │  │ │  Container  │ │    │      │\n│  │  ┌─────────────────┐    │              │  │ │nginx:1.25   │ │    │      │\n│  │  │   kubectl CLI   │    │              │  │ │Port: 80/TCP │ │    │      │\n│  │  │   (exam tool)   │    │              │  │ │             │ │    │      │\n│  │  └─────────────────┘    │              │  │ └─────────────┘ │    │      │\n│  └─────────────────────────┘              │  └─────────────────┘    │      │\n│                                           │                         │      │\n│                                           │  ┌─────────────────┐    │      │\n│                                           │  │   kube-proxy    │    │      │\n│                                           │  │ (iptables rules)│    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │   Service Layer     │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │   Service     │  │\n                          │  │front-end-svc  │  │\n                          │  │  NodePort     │  │\n                          │  │               │  │\n                          │  │ ClusterIP:    │  │\n                          │  │ 10.98.94.33   │  │\n                          │  │ Port: 80      │  │\n                          │  │               │  │\n                          │  │ NodePort:     │  │\n                          │  │ 31365         │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │   Endpoints   │  │\n                          │  │ 10.244.1.5:80 │  │\n                          │  │ (Pod IP:Port) │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Traffic Flow:\n                    ┌─────────────────────────────────┐\n                    │        External Client          │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    Node IP:31365 (NodePort)    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │   ClusterIP:80 (Service)       │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    Pod IP:80 (Container)       │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. SERVICE TYPES:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │  ClusterIP  │    │  NodePort   │    │LoadBalancer │\n   │ (Internal)  │    │(External)   │    │ (Cloud)     │\n   │ Default     │    │ Exam Focus  │    │ Advanced    │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. PORT TERMINOLOGY (CRITICAL):\n   • containerPort: Port inside the container (80)\n   • port: Service port (what clients connect to)\n   • targetPort: Container port (where traffic goes)\n   • nodePort: External port on nodes (30000-32767)\n\n3. SERVICE DISCOVERY:\n   • Services create endpoints automatically\n   • Selector matches pod labels\n   • DNS: service-name.namespace.svc.cluster.local\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 6-10 minutes maximum\n   • Edit deployment first, then create service\n   • Use kubectl expose command (faster than YAML)\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Forgetting to add containerPort to deployment\n   ❌ Wrong service name (must match question exactly)\n   ❌ Using wrong namespace\n   ❌ Incorrect port/targetPort configuration\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get deployment -n spline (check READY: 1/1)\n   ✅ kubectl get svc -n spline (check TYPE: NodePort)\n   ✅ kubectl get endpoints -n spline (check IP:PORT exists)\n   ✅ curl test both ClusterIP and NodePort\n\n4. KUBECTL SHORTCUTS (SAVE TIME):\n   • kubectl expose deployment NAME --type=NodePort --port=80\n   • kubectl patch deployment NAME -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"ports\":[{\"containerPort\":80}]}]}}}}'\n   • kubectl get svc,endpoints -n NAMESPACE\n\n📚 SERVICES & NETWORKING DOMAIN (20% of CKA):\n\n• Service Types: ClusterIP, NodePort, LoadBalancer, ExternalName\n• Ingress Controllers: nginx, traefik, istio\n• Network Policies: Pod-to-pod communication control\n• DNS: CoreDNS, service discovery, FQDN resolution\n• CNI: Container Network Interface (Flannel, Calico, etc.)\n\n🔧 TROUBLESHOOTING TIPS:\n\nService not accessible?\n→ Check: Endpoints exist, selector matches pod labels, ports correct\n\nNo endpoints?\n→ Check: Pod labels match service selector, pod is running\n\nNodePort not working?\n→ Check: Port range 30000-32767, firewall rules, node accessibility\n\nContainer port issues?\n→ Check: containerPort defined in deployment, application listening\n\n💡 EXAM DAY REMINDERS:\n\n1. kubectl expose is faster than writing YAML files\n2. Always test both ClusterIP and NodePort access\n3. Use curl or wget to verify HTTP services\n4. Check endpoints to confirm service-pod connection\n5. Remember: port (service) → targetPort (container)\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl describe svc SERVICE-NAME -n NAMESPACE\n• kubectl get endpoints SERVICE-NAME -n NAMESPACE\n• kubectl logs deployment/DEPLOYMENT-NAME -n NAMESPACE\n• kubectl get pods -n NAMESPACE -o wide (check pod IPs)"
  },
  {
    "id": "Q3",
    "title": "Ingress (L7)",
    "category": "Services & Networking",
    "difficulty": "Medium",
    "task": "Create a new Ingress resource with the following specifications:",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Flannel\n- Ingress Controller: NGINX Ingress Controller v1.8.1",
    "preparation": "# 1) NGINX Ingress Controller already installed\nkubectl get pods -n ingress-nginx  # controller running\nkubectl get ingressclasses.networking.k8s.io  # nginx class available\n\n# 2) Namespace and application already exist\nkubectl get ns sound  # namespace exists\nkubectl get deployment -n sound echoserver  # deployment exists\nkubectl get svc -n sound echoserver-service  # service exists\n\n# 3) Echoserver image will be pulled from k8s.gcr.io automatically",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Verify existing resources and check IngressClass\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n sound echoserver\nNAME         READY   UP-TO-DATE   AVAILABLE   AGE\nechoserver   1/1     1            1           5m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get svc -n sound echoserver-service\nNAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nechoserver-service   ClusterIP   10.98.192.75   <none>        8080/TCP   5m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get ingressclasses.networking.k8s.io\nNAME    CONTROLLER             PARAMETERS   AGE\nnginx   k8s.io/ingress-nginx   <none>       10m\n\n# 3) Create Ingress resource\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim ingress.yaml\n\n# Press 'i' to enter insert mode, then create:\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: echo-hello\n  namespace: sound\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: example.org\n    http:\n      paths:\n      - path: /echo-hello\n        pathType: Prefix\n        backend:\n          service:\n            name: echoserver-service\n            port:\n              number: 8080\n\n# Press Esc, then :wq to save and exit\n\n# 4) Apply the Ingress resource\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f ingress.yaml\ningress.networking.k8s.io/echo-hello created\n\n# 5) Verify Ingress creation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get ingress -n sound\nNAME         CLASS   HOSTS         ADDRESS   PORTS   AGE\necho-hello   nginx   example.org             80      30s\n\n# Check Ingress details\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe ingress echo-hello -n sound\nName:             echo-hello\nLabels:           <none>\nNamespace:        sound\nAddress:          \nIngress Class:    nginx\nDefault backend:  <default>\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  example.org  \n               /echo-hello   echoserver-service:8080 (10.244.1.9:8080)\nAnnotations:   nginx.ingress.kubernetes.io/rewrite-target: /\nEvents:\n  Type    Reason  Age   From                      Message\n  ----    ------  ----  ----                      -------\n  Normal  Sync    30s   nginx-ingress-controller  Scheduled for sync\n\n# 6) Test Ingress functionality\n# Get Ingress controller NodePort\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get svc ingress-nginx-controller -n ingress-nginx\nNAME                       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   10.97.190.138   <pending>     80:30143/TCP,443:32657/TCP   15m\n\n# Test using NodePort with Host header (exam environment method)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl -H \"Host: example.org\" http://localhost:30143/echo-hello\nCLIENT VALUES:\nclient_address=10.244.1.8\ncommand=GET\nreal path=/\nquery=nil\nrequest_version=1.1\nrequest_uri=http://example.org:8080/\n\nSERVER VALUES:\nserver_version=nginx/1.10.0\n...\n\n# Alternative test method (if curl with Host header works)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl http://example.org/echo-hello\n# Note: This requires DNS resolution or /etc/hosts entry in real exam\n\n# 7) Verify service is accessible directly (troubleshooting)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get endpoints -n sound echoserver-service\nNAME                 ENDPOINTS         AGE\nechoserver-service   10.244.1.9:8080   10m\n\n# Test service directly\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl 10.98.192.75:8080\nCLIENT VALUES:\nclient_address=10.244.0.0\ncommand=GET\nreal path=/\n...\n\n# 8) Additional verification commands\n# Check Ingress controller logs (if needed for troubleshooting)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl logs -n ingress-nginx deployment/ingress-nginx-controller --tail=10\n\n# Check all resources in sound namespace\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get all -n sound\nNAME                              READY   STATUS    RESTARTS   AGE\npod/echoserver-78c9c475c8-xxxxx   1/1     Running   0          15m\n\nNAME                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/echoserver-service   ClusterIP   10.98.192.75   <none>        8080/TCP   15m\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/echoserver   1/1     1            1           15m\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/echoserver-78c9c475c8   1         1         1       15m\n\n# 9) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ Ingress resource \"echo-hello\" created in \"sound\" namespace\n# ✅ Host-based routing configured for example.org\n# ✅ Path-based routing configured for /echo-hello\n# ✅ Backend service echoserver-service:8080 properly configured\n# ✅ HTTP access working via Ingress controller NodePort\n# ✅ URL rewrite annotation configured for proper path handling",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         Ingress & Layer 7 Routing                          │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │ Ingress Controller│  │      │\n│  │  │  - API Server   │    │              │  │   NGINX Pod     │    │      │\n│  │  │  - kubectl CLI  │    │              │  │   (ingress-nginx)│    │      │\n│  │  └─────────────────┘    │              │  │                 │    │      │\n│  └─────────────────────────┘              │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │   nginx     │ │    │      │\n│                                           │  │ │ Port: 80    │ │    │      │\n│                                           │  │ │ NodePort:   │ │    │      │\n│                                           │  │ │   30143     │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           │                         │      │\n│                                           │  ┌─────────────────┐    │      │\n│                                           │  │  Echoserver Pod │    │      │\n│                                           │  │   (sound ns)    │    │      │\n│                                           │  │                 │    │      │\n│                                           │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │ echoserver  │ │    │      │\n│                                           │  │ │ Port: 8080  │ │    │      │\n│                                           │  │ │             │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │   Ingress Layer     │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │   Ingress     │  │\n                          │  │  echo-hello   │  │\n                          │  │               │  │\n                          │  │ Host:         │  │\n                          │  │ example.org   │  │\n                          │  │               │  │\n                          │  │ Path:         │  │\n                          │  │ /echo-hello   │  │\n                          │  │               │  │\n                          │  │ Backend:      │  │\n                          │  │ echoserver-   │  │\n                          │  │ service:8080  │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │   Service     │  │\n                          │  │echoserver-svc │  │\n                          │  │ ClusterIP:    │  │\n                          │  │10.98.192.75   │  │\n                          │  │ Port: 8080    │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              L7 Traffic Flow:\n                    ┌─────────────────────────────────┐\n                    │        External Client          │\n                    │  curl http://example.org/       │\n                    │       echo-hello                │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    Node IP:30143 (NodePort)    │\n                    │      Ingress Controller         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │      Host: example.org          │\n                    │      Path: /echo-hello          │\n                    │    (L7 HTTP Routing)            │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │   Service: echoserver-service   │\n                    │      ClusterIP:8080             │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    Pod: echoserver:8080         │\n                    │    (Rewrite to /)               │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. INGRESS vs SERVICE COMPARISON:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │   Service   │    │   Ingress   │    │   Gateway   │\n   │ (Layer 4)   │    │ (Layer 7)   │    │ (Future)    │\n   │ IP:Port     │    │ Host+Path   │    │ Advanced    │\n   │ NodePort    │    │ HTTP Rules  │    │ API         │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. INGRESS COMPONENTS (CRITICAL):\n   • IngressClass: Defines which controller to use\n   • Ingress Resource: Defines routing rules\n   • Ingress Controller: Implements the rules (nginx, traefik, etc.)\n   • Backend Service: Target service for traffic\n\n3. PATH TYPES:\n   • Exact: Matches exact path only\n   • Prefix: Matches path prefix (most common)\n   • ImplementationSpecific: Controller-specific\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 8-12 minutes maximum\n   • Check IngressClass first (kubectl get ingressclasses)\n   • Use vim efficiently for YAML creation\n   • Test with curl and Host header\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Wrong ingressClassName (must match existing class)\n   ❌ Missing rewrite-target annotation\n   ❌ Incorrect host or path specification\n   ❌ Wrong service name or port number\n   ❌ Forgetting pathType field\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get ingressclasses (check available classes)\n   ✅ kubectl get ingress -n NAMESPACE (check creation)\n   ✅ kubectl describe ingress NAME -n NAMESPACE (check rules)\n   ✅ curl test with Host header\n   ✅ Check service endpoints exist\n\n4. KUBECTL SHORTCUTS (SAVE TIME):\n   • kubectl get ing -A (list all ingresses)\n   • kubectl describe ing NAME -n NS (detailed info)\n   • kubectl get svc -n ingress-nginx (controller service)\n   • curl -H \"Host: example.org\" http://localhost:PORT/path\n\n📚 SERVICES & NETWORKING DOMAIN (20% of CKA):\n\n• Ingress Controllers: nginx, traefik, istio, ambassador\n• HTTP Routing: Host-based, path-based, header-based\n• TLS Termination: SSL certificates, HTTPS redirection\n• Annotations: Controller-specific configurations\n• Load Balancing: Round-robin, session affinity\n\n🔧 TROUBLESHOOTING TIPS:\n\nIngress not working?\n→ Check: IngressClass exists, controller running, service exists\n\n404 errors?\n→ Check: Path configuration, rewrite-target annotation, backend service\n\nHost resolution issues?\n→ Check: Host header in curl, DNS configuration, /etc/hosts\n\nBackend connection failed?\n→ Check: Service endpoints, pod status, port configuration\n\n💡 EXAM DAY REMINDERS:\n\n1. Always check existing IngressClass before creating Ingress\n2. Use Host header with curl for testing: curl -H \"Host: example.org\"\n3. Verify service exists and has endpoints before creating Ingress\n4. Common annotation: nginx.ingress.kubernetes.io/rewrite-target: /\n5. PathType is required field in networking.k8s.io/v1\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get ingressclasses.networking.k8s.io\n• kubectl describe ingress NAME -n NAMESPACE\n• kubectl get endpoints SERVICE-NAME -n NAMESPACE\n• kubectl logs -n ingress-nginx deployment/ingress-nginx-controller\n• kubectl get svc -n ingress-nginx (check controller service)\n\n📋 INGRESS YAML TEMPLATE:\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: INGRESS-NAME\n  namespace: NAMESPACE\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: HOSTNAME\n    http:\n      paths:\n      - path: /PATH\n        pathType: Prefix\n        backend:\n          service:\n            name: SERVICE-NAME\n            port:\n              number: PORT\n```"
  },
  {
    "id": "Q4",
    "title": "NetworkPolicy",
    "category": "Services & Networking",
    "difficulty": "Hard",
    "task": "Select and apply the appropriate NetworkPolicy from the provided YAML files. Ensure that the selected NetworkPolicy is not overly permissive while allowing communication between the frontend and backend Deployments running in the frontend and backend namespaces.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico v3.26.1 (for NetworkPolicy enforcement)\n- Original CNI: Flannel (still present)",
    "preparation": "# 1) Namespaces and deployments already exist\nkubectl get ns frontend backend  # namespaces exist\nkubectl get deployment -n frontend frontend-app  # frontend deployment exists\nkubectl get deployment -n backend backend-app   # backend deployment exists\n\n# 2) Default deny-all NetworkPolicy already applied\nkubectl get networkpolicies -n backend  # default-deny-all exists\n\n# 3) NetworkPolicy examples available in ~/netpol directory\nls ~/netpol/  # netpol1.yaml, netpol2.yaml, netpol3.yaml\n\n# 4) Calico CNI installed for NetworkPolicy enforcement\nkubectl get pods -n kube-system | grep calico  # calico components running",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Analyze existing deployments and their labels\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get ns frontend backend --show-labels\nNAME       STATUS   AGE   LABELS\nfrontend   Active   5m    kubernetes.io/metadata.name=frontend\nbackend    Active   5m    kubernetes.io/metadata.name=backend\n\n# Check pod labels in both namespaces\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl -n frontend get pods --show-labels\nNAME                            READY   STATUS    RESTARTS   AGE   LABELS\nfrontend-app-76948cb864-xxxxx   1/1     Running   0          5m    app=frontend,pod-template-hash=76948cb864\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl -n backend get pods --show-labels\nNAME                           READY   STATUS    RESTARTS   AGE   LABELS\nbackend-app-5c9d8798c6-xxxxx   1/1     Running   0          5m    app=backend,pod-template-hash=5c9d8798c6\n\n# 3) Check existing NetworkPolicies\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl -n backend get networkpolicies\nNAME               POD-SELECTOR   AGE\ndefault-deny-all   <none>         5m\n\n# Verify the default deny policy blocks all ingress traffic\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe networkpolicy default-deny-all -n backend\nName:         default-deny-all\nNamespace:    backend\nCreated on:   2025-08-09 09:18:00 +0000 UTC\nLabels:       <none>\nAnnotations:  <none>\nSpec:\n  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)\n  Allowing ingress traffic:\n    <none> (Selected pods are isolated for ingress connectivity)\n  Not affecting egress traffic\n  Policy Types: Ingress\n\n# 4) Examine NetworkPolicy examples in ~/netpol directory\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ ls ~/netpol/\nnetpol1.yaml  netpol2.yaml  netpol3.yaml\n\n# Analyze each policy to determine the most appropriate one:\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat ~/netpol/netpol1.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: netpol-1\n  namespace: backend\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: frontend\n\n# Analysis: This policy allows ALL pods in frontend namespace to access ALL pods in backend namespace\n# This is OVERLY PERMISSIVE - not recommended\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat ~/netpol/netpol2.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: netpol-2\n  namespace: backend\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: frontend\n      podSelector:\n        matchLabels:\n          app: frontend\n\n# Analysis: This policy allows ONLY pods with app=frontend label in frontend namespace\n# to access ONLY pods with app=backend label in backend namespace\n# This is APPROPRIATE - follows principle of least privilege\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat ~/netpol/netpol3.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: netpol-3\n  namespace: backend\nspec:\n  podSelector:\n    matchLabels:\n      app: test\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: frontend\n      podSelector:\n        matchLabels:\n          app: frontend\n    - ipBlock:\n        cidr: 10.244.0.0/24\n\n# Analysis: This policy targets pods with app=test label, but our backend pods have app=backend\n# This policy is NOT APPLICABLE to our scenario\n\n# 5) Apply the appropriate NetworkPolicy (netpol2.yaml)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f ~/netpol/netpol2.yaml\nnetworkpolicy.networking.k8s.io/netpol-2 created\n\n# 6) Verify NetworkPolicy application\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get networkpolicies -n backend\nNAME               POD-SELECTOR   AGE\ndefault-deny-all   <none>         10m\nnetpol-2           app=backend    30s\n\n# Check detailed policy configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe networkpolicy netpol-2 -n backend\nName:         netpol-2\nNamespace:    backend\nCreated on:   2025-08-09 09:21:31 +0000 UTC\nLabels:       <none>\nAnnotations:  <none>\nSpec:\n  PodSelector:     app=backend\n  Allowing ingress traffic:\n    To Port: <any> (traffic allowed to all ports)\n    From:\n      NamespaceSelector: kubernetes.io/metadata.name=frontend\n      PodSelector: app=frontend\n  Not affecting egress traffic\n  Policy Types: Ingress\n\n# 7) Test connectivity to verify policy effectiveness\n# Get backend pod IP\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n backend -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE\nbackend-app-5c9d8798c6-xxxxx   1/1     Running   0          15m   10.244.1.11   ec2-54-144-18-63.compute-1.amazonaws.com\n\n# Test connection from frontend to backend (should work)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl exec -n frontend deployment/frontend-app -- curl -m 5 10.244.1.11\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n\n# 8) Additional verification (optional)\n# Verify that the policy is selective by checking pod labels match\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n frontend -l app=frontend\nNAME                            READY   STATUS    RESTARTS   AGE\nfrontend-app-76948cb864-xxxxx   1/1     Running   0          20m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n backend -l app=backend\nNAME                           READY   STATUS    RESTARTS   AGE\nbackend-app-5c9d8798c6-xxxxx   1/1     Running   0          20m\n\n# 9) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ Analyzed frontend and backend deployments and their labels\n# ✅ Examined all three NetworkPolicy examples in ~/netpol directory\n# ✅ Selected netpol2.yaml as the most appropriate (not overly permissive)\n# ✅ Applied the NetworkPolicy without modifying existing default-deny-all policy\n# ✅ Verified frontend-backend communication works as expected\n# ✅ Maintained principle of least privilege with specific pod and namespace selectors",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         NetworkPolicy & Micro-segmentation                 │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │  Frontend Pod   │    │      │\n│  │  │  - API Server   │    │              │  │ (frontend ns)   │    │      │\n│  │  │  - kubectl CLI  │    │              │  │                 │    │      │\n│  │  └─────────────────┘    │              │  │ ┌─────────────┐ │    │      │\n│  │                         │              │  │ │   nginx     │ │    │      │\n│  │  ┌─────────────────┐    │              │  │ │app=frontend │ │    │      │\n│  │  │ Calico Controller│    │              │  │ │Port: 80     │ │    │      │\n│  │  │ (NetworkPolicy  │    │              │  │ └─────────────┘ │    │      │\n│  │  │  Enforcement)   │    │              │  └─────────────────┘    │      │\n│  │  └─────────────────┘    │              │                         │      │\n│  └─────────────────────────┘              │  ┌─────────────────┐    │      │\n│                                           │  │  Backend Pod    │    │      │\n│                                           │  │  (backend ns)   │    │      │\n│                                           │  │                 │    │      │\n│                                           │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │   nginx     │ │    │      │\n│                                           │  │ │app=backend  │ │    │      │\n│                                           │  │ │Port: 80     │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           │                         │      │\n│                                           │  ┌─────────────────┐    │      │\n│                                           │  │  Calico Node    │    │      │\n│                                           │  │ (Policy Agent)  │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │ NetworkPolicy Layer │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │default-deny-all│  │\n                          │  │  (backend ns) │  │\n                          │  │               │  │\n                          │  │ podSelector:  │  │\n                          │  │     {}        │  │\n                          │  │ policyTypes:  │  │\n                          │  │   - Ingress   │  │\n                          │  │ ingress: []   │  │\n                          │  │ (DENY ALL)    │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │   netpol-2    │  │\n                          │  │  (backend ns) │  │\n                          │  │               │  │\n                          │  │ podSelector:  │  │\n                          │  │ app=backend   │  │\n                          │  │               │  │\n                          │  │ Allow from:   │  │\n                          │  │ frontend ns + │  │\n                          │  │ app=frontend  │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Traffic Flow Analysis:\n                    ┌─────────────────────────────────┐\n                    │     Frontend Pod                │\n                    │   (app=frontend label)         │\n                    │   in frontend namespace        │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    NetworkPolicy Evaluation    │\n                    │                                 │\n                    │ 1. default-deny-all: DENY      │\n                    │ 2. netpol-2: ALLOW             │\n                    │    (more specific wins)        │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │     Backend Pod                 │\n                    │   (app=backend label)          │\n                    │   in backend namespace         │\n                    │     ✅ ACCESS GRANTED          │\n                    └─────────────────────────────────┘\n\n                              Policy Comparison:\n                    ┌─────────────────────────────────┐\n                    │         netpol1.yaml           │\n                    │    (OVERLY PERMISSIVE)         │\n                    │                                 │\n                    │ Allows: ALL frontend pods      │\n                    │      -> ALL backend pods       │\n                    │ Risk: Too broad access         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         netpol2.yaml           │\n                    │      (APPROPRIATE) ✅          │\n                    │                                 │\n                    │ Allows: app=frontend pods      │\n                    │      -> app=backend pods       │\n                    │ Principle: Least privilege     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         netpol3.yaml           │\n                    │     (NOT APPLICABLE)           │\n                    │                                 │\n                    │ Targets: app=test pods         │\n                    │ Problem: No matching pods      │\n                    │ Result: Policy has no effect   │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. NETWORKPOLICY COMPONENTS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │podSelector  │    │policyTypes  │    │ ingress/    │\n   │(target pods)│    │(Ingress/    │    │ egress      │\n   │             │    │ Egress)     │    │ (rules)     │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. SELECTOR TYPES (CRITICAL):\n   • podSelector: Selects pods within the same namespace\n   • namespaceSelector: Selects entire namespaces\n   • Combined: namespaceSelector + podSelector (AND operation)\n   • ipBlock: Selects IP ranges (CIDR blocks)\n\n3. POLICY EVALUATION:\n   • Default: All traffic allowed (no NetworkPolicies)\n   • With NetworkPolicy: Default deny + explicit allow rules\n   • Multiple policies: Union of all allow rules\n   • Precedence: More specific policies take effect\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 10-15 minutes maximum\n   • Read all policy files first before deciding\n   • Analyze pod and namespace labels carefully\n   • Apply policy and verify with kubectl describe\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Choosing overly permissive policy (netpol1 type)\n   ❌ Applying policy that doesn't match existing labels\n   ❌ Modifying or deleting existing default-deny policy\n   ❌ Not understanding AND vs OR in selectors\n   ❌ Forgetting to check policy effectiveness\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get ns --show-labels (check namespace labels)\n   ✅ kubectl get pods -n NS --show-labels (check pod labels)\n   ✅ kubectl get networkpolicies -n NS (check existing policies)\n   ✅ kubectl describe networkpolicy NAME -n NS (verify rules)\n   ✅ Test connectivity with kubectl exec ... curl\n\n4. ANALYSIS FRAMEWORK:\n   • Step 1: Identify source (frontend) and target (backend) labels\n   • Step 2: Examine each policy's podSelector and ingress rules\n   • Step 3: Choose policy that matches labels without being overly broad\n   • Step 4: Apply and verify policy takes effect\n\n📚 SERVICES & NETWORKING DOMAIN (20% of CKA):\n\n• NetworkPolicy Types: Ingress, Egress, Both\n• Selectors: podSelector, namespaceSelector, ipBlock\n• Policy Evaluation: Default allow → Default deny with explicit allows\n• CNI Requirements: Calico, Cilium, Weave (not all CNIs support NetworkPolicies)\n• Use Cases: Micro-segmentation, compliance, security isolation\n\n🔧 TROUBLESHOOTING TIPS:\n\nPolicy not working?\n→ Check: CNI supports NetworkPolicies, labels match exactly, policy applied to correct namespace\n\nTraffic still blocked?\n→ Check: Multiple policies (union of rules), default-deny policies, pod readiness\n\nOverly permissive?\n→ Check: Empty podSelector {} affects all pods, missing label selectors\n\nLabels don't match?\n→ Check: kubectl get pods --show-labels, case sensitivity, exact matches required\n\n💡 EXAM DAY REMINDERS:\n\n1. Always check existing labels before choosing policy\n2. Avoid overly permissive policies (empty selectors)\n3. Don't modify existing default-deny policies\n4. Test policy effectiveness with kubectl exec curl\n5. Remember: namespaceSelector + podSelector = AND operation\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get ns --show-labels\n• kubectl get pods -n NAMESPACE --show-labels\n• kubectl get networkpolicies -A\n• kubectl describe networkpolicy NAME -n NAMESPACE\n• kubectl exec -n NS deployment/NAME -- curl IP\n\n📋 NETWORKPOLICY ANALYSIS TEMPLATE:\n\n```yaml\n# Policy Analysis Checklist:\n# 1. What pods does podSelector target?\n# 2. What traffic does policyTypes control?\n# 3. What sources do ingress rules allow?\n# 4. Do selectors match existing labels?\n# 5. Is policy appropriately restrictive?\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: POLICY-NAME\n  namespace: TARGET-NAMESPACE\nspec:\n  podSelector:\n    matchLabels:\n      app: TARGET-LABEL\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: SOURCE-NAMESPACE\n      podSelector:\n        matchLabels:\n          app: SOURCE-LABEL\n```\n\n🎯 POLICY SELECTION CRITERIA:\n\n✅ **GOOD POLICY (netpol2 type)**:\n- Specific pod selectors (app=backend)\n- Specific source selectors (app=frontend)\n- Follows principle of least privilege\n- Matches existing pod labels\n\n❌ **BAD POLICY (netpol1 type)**:\n- Empty pod selector (affects all pods)\n- Only namespace selector (too broad)\n- Overly permissive access\n- Violates security best practices\n\n❌ **IRRELEVANT POLICY (netpol3 type)**:\n- Selectors don't match existing labels\n- Policy has no effect\n- Wastes exam time"
  },
  {
    "id": "Q5",
    "title": "ConfigMap",
    "category": "Workloads & Scheduling",
    "difficulty": "Medium",
    "task": "The NGINX Deployment named nginx-static-test is running in the nginx-static-test namespace. It is configured through a ConfigMap named nginx-static-config.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) Namespace and resources already exist\nkubectl get ns nginx-static-test  # namespace exists\nkubectl get configmap -n nginx-static-test nginx-static-config  # ConfigMap exists\nkubectl get deployment -n nginx-static-test nginx-static-test   # deployment exists\n\n# 2) SSL certificates and service already configured\nkubectl get secret -n nginx-static-test nginx-ssl-secret  # SSL certificates\nkubectl get svc -n nginx-static-test nginx-static-service # service for testing\n\n# 3) Current configuration uses TLSv1.2 (needs to be updated to TLSv1.3)",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Verify existing resources\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n nginx-static-test nginx-static-test\nNAME                READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-static-test   1/1     1            1           5m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get configmap -n nginx-static-test nginx-static-config\nNAME                  DATA   AGE\nnginx-static-config   1      5m\n\n# 3) Export existing ConfigMap to a file for editing\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get configmap nginx-static-config -n nginx-static-test -o yaml > nginx-static-config.yaml\n\n# 4) Edit the ConfigMap to update SSL protocols\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim nginx-static-config.yaml\n\n# Find the ssl_protocols line and change it:\n# FROM: ssl_protocols TLSv1.2;\n# TO:   ssl_protocols TLSv1.3;\n\n# The relevant section should look like:\n# data:\n#   nginx.conf: |\n#     worker_processes 1;\n#     events {\n#       worker_connections 1024;\n#     }\n#     http {\n#       # SSL configuration\n#       ssl_protocols TLSv1.3;  # Changed from TLSv1.2 to TLSv1.3\n#       ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384';\n#       ssl_prefer_server_ciphers on;\n#       \n#       server {\n#         listen 80;\n#         listen 443 ssl;\n#         server_name web.k8snginx.local;\n#         \n#         ssl_certificate /etc/nginx/ssl/tls.crt;\n#         ssl_certificate_key /etc/nginx/ssl/tls.key;\n#         \n#         location / {\n#           root /usr/share/nginx/html;\n#           index index.html index.htm;\n#         }\n#       }\n#     }\n\n# Save and exit (:wq in vim)\n\n# Alternative method using sed (faster for exam):\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ sed -i 's/ssl_protocols TLSv1.2;/ssl_protocols TLSv1.3;/' nginx-static-config.yaml\n\n# 5) Apply the updated ConfigMap\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f nginx-static-config.yaml\nconfigmap/nginx-static-config configured\n\n# 6) Verify the ConfigMap was updated\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get configmap nginx-static-config -n nginx-static-test -o jsonpath='{.data.nginx\\.conf}' | grep ssl_protocols\n  ssl_protocols TLSv1.3;\n\n# 7) Restart the deployment to apply the new configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl rollout restart deployment nginx-static-test -n nginx-static-test\ndeployment.apps/nginx-static-test restarted\n\n# 8) Wait for the rollout to complete\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl rollout status deployment nginx-static-test -n nginx-static-test\ndeployment \"nginx-static-test\" successfully rolled out\n\n# 9) Verify the new pod is running\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n nginx-static-test\nNAME                                 READY   STATUS    RESTARTS   AGE\nnginx-static-test-7c4dcc5dd7-xxxxx   1/1     Running   0          30s\n\n# 10) Test the TLS configuration\n# Get the service IP for testing\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get svc nginx-static-service -n nginx-static-test\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nnginx-static-service   ClusterIP   10.111.223.97   <none>        80/TCP,443/TCP   10m\n\n# Test TLSv1.2 connection (should fail now)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl -k --tls-max 1.2 https://10.111.223.97\ncurl: (35) error:0A00042E:SSL routines::tlsv1 alert protocol version\n\n# Test TLSv1.3 connection (should work)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ curl -k --tls-max 1.3 https://10.111.223.97\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n\n# 11) Additional verification (optional)\n# Check that the ConfigMap change is reflected in the pod\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl exec -n nginx-static-test deployment/nginx-static-test -- grep ssl_protocols /etc/nginx/nginx.conf\n  ssl_protocols TLSv1.3;\n\n# 12) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ ConfigMap nginx-static-config updated to only allow TLSv1.3\n# ✅ Deployment restarted to apply new configuration\n# ✅ TLSv1.2 connections now fail (protocol version error)\n# ✅ TLSv1.3 connections work properly\n# ✅ Configuration change verified in running pod",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         ConfigMap & Configuration Management                │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │   nginx Pod     │    │      │\n│  │  │  - API Server   │    │              │  │(nginx-static-   │    │      │\n│  │  │  - etcd         │    │              │  │    test ns)     │    │      │\n│  │  │  - kubectl CLI  │    │              │  │                 │    │      │\n│  │  └─────────────────┘    │              │  │ ┌─────────────┐ │    │      │\n│  └─────────────────────────┘              │  │ │   nginx     │ │    │      │\n│                                           │  │ │ Port: 80    │ │    │      │\n│                                           │  │ │ Port: 443   │ │    │      │\n│                                           │  │ │ (TLSv1.3)   │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │ Configuration Layer │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │   ConfigMap   │  │\n                          │  │nginx-static-  │  │\n                          │  │    config     │  │\n                          │  │               │  │\n                          │  │ Data:         │  │\n                          │  │ nginx.conf    │  │\n                          │  │               │  │\n                          │  │ Contains:     │  │\n                          │  │ ssl_protocols │  │\n                          │  │   TLSv1.3     │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │    Secret     │  │\n                          │  │nginx-ssl-     │  │\n                          │  │   secret      │  │\n                          │  │               │  │\n                          │  │ Data:         │  │\n                          │  │ tls.crt       │  │\n                          │  │ tls.key       │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Volume Mounts:\n                    ┌─────────────────────────────────┐\n                    │         nginx Pod               │\n                    │                                 │\n                    │  /etc/nginx/nginx.conf         │\n                    │  ← ConfigMap (nginx.conf)      │\n                    │                                 │\n                    │  /etc/nginx/ssl/               │\n                    │  ← Secret (tls.crt, tls.key)  │\n                    └─────────────────────────────────┘\n\n                              Configuration Flow:\n                    ┌─────────────────────────────────┐\n                    │    1. Update ConfigMap          │\n                    │   (ssl_protocols TLSv1.3)      │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. Restart Deployment        │\n                    │   (kubectl rollout restart)    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. New Pod Created           │\n                    │   (with updated config)        │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. TLS Configuration         │\n                    │   (TLSv1.2 ❌, TLSv1.3 ✅)    │\n                    └─────────────────────────────────┘\n\n                              Before vs After:\n                    ┌─────────────────────────────────┐\n                    │         BEFORE UPDATE          │\n                    │                                 │\n                    │ ssl_protocols TLSv1.2;         │\n                    │                                 │\n                    │ TLSv1.2 connections: ✅        │\n                    │ TLSv1.3 connections: ✅        │\n                    │                                 │\n                    │ Security: Moderate              │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         AFTER UPDATE           │\n                    │                                 │\n                    │ ssl_protocols TLSv1.3;         │\n                    │                                 │\n                    │ TLSv1.2 connections: ❌        │\n                    │ TLSv1.3 connections: ✅        │\n                    │                                 │\n                    │ Security: Enhanced              │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. CONFIGMAP USAGE PATTERNS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │Environment  │    │Volume Mount │    │Command Args │\n   │Variables    │    │(Files)      │    │(Parameters) │\n   │env/envFrom  │    │volumeMounts │    │args/command │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. CONFIGMAP UPDATE METHODS (CRITICAL):\n   • kubectl edit configmap NAME -n NAMESPACE (direct edit)\n   • kubectl apply -f updated-configmap.yaml (file-based)\n   • kubectl patch configmap NAME --patch '...' (patch-based)\n   • kubectl create configmap --dry-run=client -o yaml (generate)\n\n3. DEPLOYMENT UPDATE STRATEGIES:\n   • Rolling Update: Default, zero-downtime updates\n   • Recreate: All pods terminated, then new ones created\n   • Manual Restart: kubectl rollout restart deployment NAME\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 8-12 minutes maximum\n   • Export ConfigMap first (kubectl get cm NAME -o yaml > file.yaml)\n   • Use sed for simple text replacements (faster than vim)\n   • Always restart deployment after ConfigMap changes\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Forgetting to restart deployment after ConfigMap update\n   ❌ Editing ConfigMap directly without backing up\n   ❌ Not verifying the change took effect in the pod\n   ❌ Incorrect YAML indentation when editing\n   ❌ Not testing the actual functionality\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get configmap NAME -o yaml (verify ConfigMap updated)\n   ✅ kubectl rollout status deployment NAME (verify restart completed)\n   ✅ kubectl get pods (verify new pod is running)\n   ✅ kubectl exec ... -- cat /path/to/config (verify file in pod)\n   ✅ Test actual functionality (curl, etc.)\n\n4. EFFICIENT WORKFLOW:\n   • Step 1: Export ConfigMap to file\n   • Step 2: Edit file (vim or sed)\n   • Step 3: Apply updated ConfigMap\n   • Step 4: Restart deployment\n   • Step 5: Verify and test\n\n📚 WORKLOADS & SCHEDULING DOMAIN (15% of CKA):\n\n• ConfigMaps: Configuration data storage and injection\n• Secrets: Sensitive data storage (certificates, passwords)\n• Volume Mounts: File-based configuration injection\n• Environment Variables: Key-value configuration injection\n• Rolling Updates: Zero-downtime configuration updates\n\n🔧 TROUBLESHOOTING TIPS:\n\nConfigMap changes not applied?\n→ Check: Deployment restarted, pod recreated, volume mount correct\n\nPod failing to start after config change?\n→ Check: Configuration syntax, file paths, permissions\n\nTLS/SSL not working?\n→ Check: Certificate paths, protocol versions, cipher suites\n\nConfiguration file not found in pod?\n→ Check: Volume mount path, subPath specification, ConfigMap key name\n\n💡 EXAM DAY REMINDERS:\n\n1. Always backup ConfigMap before editing (export to file)\n2. Restart deployment after ConfigMap changes (not automatic)\n3. Use kubectl rollout status to verify restart completion\n4. Test actual functionality, not just resource status\n5. subPath is required when mounting single files from ConfigMap\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get configmap NAME -o yaml\n• kubectl describe configmap NAME\n• kubectl exec POD -- cat /path/to/config/file\n• kubectl rollout history deployment NAME\n• kubectl logs deployment/NAME\n\n📋 CONFIGMAP UPDATE WORKFLOW:\n\n```bash\n# 1. Export existing ConfigMap\nkubectl get configmap NAME -n NAMESPACE -o yaml > config.yaml\n\n# 2. Edit configuration (vim or sed)\nvim config.yaml\n# OR\nsed -i 's/old-value/new-value/' config.yaml\n\n# 3. Apply updated ConfigMap\nkubectl apply -f config.yaml\n\n# 4. Restart deployment\nkubectl rollout restart deployment NAME -n NAMESPACE\n\n# 5. Verify rollout\nkubectl rollout status deployment NAME -n NAMESPACE\n\n# 6. Test functionality\nkubectl exec deployment/NAME -- cat /path/to/config\n```\n\n🎯 CONFIGMAP MOUNT PATTERNS:\n\n✅ **VOLUME MOUNT (File-based)**:\n```yaml\nvolumeMounts:\n- name: config-volume\n  mountPath: /etc/nginx/nginx.conf\n  subPath: nginx.conf\nvolumes:\n- name: config-volume\n  configMap:\n    name: nginx-config\n```\n\n✅ **ENVIRONMENT VARIABLES**:\n```yaml\nenv:\n- name: CONFIG_VALUE\n  valueFrom:\n    configMapKeyRef:\n      name: app-config\n      key: config.value\n```\n\n✅ **ENTIRE CONFIGMAP AS ENV**:\n```yaml\nenvFrom:\n- configMapRef:\n    name: app-config\n```\n\n🔧 TLS/SSL CONFIGURATION TIPS:\n\n• ssl_protocols: Specify allowed TLS versions\n• ssl_ciphers: Define allowed cipher suites\n• ssl_prefer_server_ciphers: Server cipher preference\n• ssl_certificate/ssl_certificate_key: Certificate file paths\n• Test with: curl -k --tls-max X.X https://endpoint"
  },
  {
    "id": "Q6",
    "title": "StorageClass",
    "category": "Storage",
    "difficulty": "Easy",
    "task": "Task:\nFirst, create a new StorageClass named test-local-path for the existing provisioner named rancher.io/local-path.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) Local-path-provisioner already installed\nkubectl get deployment -n local-path-storage  # provisioner running\nkubectl get storageclass  # existing StorageClasses visible\n\n# 2) Rancher local-path provisioner available\n# Provisioner: rancher.io/local-path\n# Namespace: local-path-storage\n\n# 3) Current default StorageClass will be replaced",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Check existing StorageClasses and provisioner\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get storageclass\nNAME         PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-path   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  45m\n\n# Verify local-path-provisioner is running\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n local-path-storage\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\nlocal-path-provisioner   1/1     1            1           5m\n\n# 3) Create the test-local-path StorageClass\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim sc.yaml\n\n# Press 'i' to enter insert mode, then create:\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: test-local-path\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: rancher.io/local-path\nvolumeBindingMode: WaitForFirstConsumer\nreclaimPolicy: Delete\n\n# Press Esc, then :wq to save and exit\n\n# 4) Apply the StorageClass\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f sc.yaml\nstorageclass.storage.k8s.io/test-local-path created\n\n# 5) Verify StorageClass creation and default status\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get storageclass\nNAME                        PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-path                  kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  46m\ntest-local-path (default)   rancher.io/local-path          Delete          WaitForFirstConsumer   false                  10s\n\n# Note: The \"(default)\" indicator shows test-local-path is now the default StorageClass\n\n# 6) Verify StorageClass details\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe storageclass test-local-path\nName:            test-local-path\nIsDefaultClass:  Yes\nAnnotations:     storageclass.kubernetes.io/is-default-class=true\nProvisioner:           rancher.io/local-path\nParameters:            <none>\nAllowVolumeExpansion:  <unset>\nMountOptions:          <none>\nReclaimPolicy:         Delete\nVolumeBindingMode:     WaitForFirstConsumer\nEvents:                <none>\n\n# 7) Verify the annotation is correctly set\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get storageclass test-local-path -o yaml | grep is-default-class\n    storageclass.kubernetes.io/is-default-class: \"true\"\n\n# 8) Test StorageClass functionality (optional verification)\n# Create a test PVC without specifying storageClassName (should use default)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n\n# Check PVC status (should be Pending due to WaitForFirstConsumer)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pvc test-pvc\nNAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE\ntest-pvc   Pending                                      test-local-path   5s\n\n# Verify PVC uses the default StorageClass\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe pvc test-pvc | grep StorageClass\nStorageClass:  test-local-path\n\n# Clean up test PVC\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl delete pvc test-pvc\npersistentvolumeclaim \"test-pvc\" deleted\n\n# 9) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ StorageClass \"test-local-path\" created with correct name\n# ✅ Provisioner \"rancher.io/local-path\" configured correctly\n# ✅ VolumeBindingMode set to \"WaitForFirstConsumer\" as required\n# ✅ StorageClass configured as default with proper annotation\n# ✅ No existing Deployments or PVCs were modified\n# ✅ Dynamic provisioning functionality verified",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         StorageClass & Dynamic Provisioning                │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │Local-Path       │    │      │\n│  │  │  - API Server   │    │              │  │Provisioner Pod  │    │      │\n│  │  │  - etcd         │    │              │  │                 │    │      │\n│  │  │  - kubectl CLI  │    │              │  │ ┌─────────────┐ │    │      │\n│  │  └─────────────────┘    │              │  │ │Provisioner  │ │    │      │\n│  └─────────────────────────┘              │  │ │rancher.io/  │ │    │      │\n│                                           │  │ │local-path   │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           │                         │      │\n│                                           │  ┌─────────────────┐    │      │\n│                                           │  │  Application    │    │      │\n│                                           │  │     Pods        │    │      │\n│                                           │  │                 │    │      │\n│                                           │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │   Pod +     │ │    │      │\n│                                           │  │ │ PVC Mount   │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │ Storage Layer       │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │ StorageClass  │  │\n                          │  │test-local-path│  │\n                          │  │  (DEFAULT)    │  │\n                          │  │               │  │\n                          │  │ Provisioner:  │  │\n                          │  │rancher.io/    │  │\n                          │  │ local-path    │  │\n                          │  │               │  │\n                          │  │ VolumeBinding:│  │\n                          │  │WaitForFirst   │  │\n                          │  │ Consumer      │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │      PVC      │  │\n                          │  │   (Pending)   │  │\n                          │  │               │  │\n                          │  │ Uses default  │  │\n                          │  │ StorageClass  │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │      PV       │  │\n                          │  │  (Created     │  │\n                          │  │ dynamically)  │  │\n                          │  │               │  │\n                          │  │ Local storage │  │\n                          │  │ on worker     │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Dynamic Provisioning Flow:\n                    ┌─────────────────────────────────┐\n                    │    1. PVC Created               │\n                    │   (no storageClassName)        │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. Default StorageClass      │\n                    │   (test-local-path) Selected    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. PVC Status: Pending       │\n                    │   (WaitForFirstConsumer)       │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Pod Scheduled             │\n                    │   (with PVC mount)             │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    5. Provisioner Creates PV    │\n                    │   (local storage on node)      │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    6. PVC Status: Bound         │\n                    │   (PV attached to PVC)         │\n                    └─────────────────────────────────┘\n\n                              Volume Binding Modes:\n                    ┌─────────────────────────────────┐\n                    │        Immediate                │\n                    │                                 │\n                    │ PVC → PV created immediately    │\n                    │ Pod scheduled to any node       │\n                    │ Risk: Pod may not fit on node   │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    WaitForFirstConsumer ✅     │\n                    │                                 │\n                    │ PVC → Pending until pod         │\n                    │ Pod scheduled → PV created      │\n                    │ Benefit: Optimal node placement │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. STORAGECLASS COMPONENTS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │Provisioner  │    │VolumeBinding│    │ReclaimPolicy│\n   │(Required)   │    │Mode         │    │(Optional)   │\n   │             │    │(Important)  │    │             │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. VOLUME BINDING MODES (CRITICAL):\n   • Immediate: PV created immediately when PVC is created\n   • WaitForFirstConsumer: PV created when first pod using PVC is scheduled\n   • WaitForFirstConsumer is often preferred for node-local storage\n\n3. DEFAULT STORAGECLASS:\n   • Only one StorageClass can be default at a time\n   • Annotation: storageclass.kubernetes.io/is-default-class: \"true\"\n   • Used when PVC doesn't specify storageClassName\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 5-8 minutes maximum\n   • Simple YAML creation and application\n   • Verify default status with kubectl get storageclass\n   • Test with a simple PVC if time permits\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Wrong provisioner name (must match exactly)\n   ❌ Incorrect volumeBindingMode (question specifies WaitForFirstConsumer)\n   ❌ Missing default annotation\n   ❌ Modifying existing resources (explicitly forbidden)\n   ❌ Not verifying the StorageClass is actually default\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get storageclass (check for \"(default)\" indicator)\n   ✅ kubectl describe storageclass NAME (verify all fields)\n   ✅ kubectl get storageclass NAME -o yaml (check annotations)\n   ✅ Create test PVC without storageClassName (should use default)\n\n4. YAML TEMPLATE:\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: REQUIRED-NAME\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: EXACT-PROVISIONER-NAME\nvolumeBindingMode: WaitForFirstConsumer\nreclaimPolicy: Delete\n```\n\n📚 STORAGE DOMAIN (10% of CKA):\n\n• StorageClass: Defines storage provisioning policies\n• Dynamic Provisioning: Automatic PV creation based on PVC requests\n• Provisioners: CSI drivers, cloud providers, local storage\n• Volume Binding: Immediate vs WaitForFirstConsumer\n• Reclaim Policies: Delete, Retain, Recycle\n\n🔧 TROUBLESHOOTING TIPS:\n\nStorageClass not working?\n→ Check: Provisioner exists, correct provisioner name, RBAC permissions\n\nPVC stuck in Pending?\n→ Check: StorageClass exists, provisioner running, node resources\n\nDefault StorageClass not used?\n→ Check: Only one default exists, annotation correct, PVC has no storageClassName\n\nVolume binding issues?\n→ Check: VolumeBindingMode, node affinity, pod scheduling constraints\n\n💡 EXAM DAY REMINDERS:\n\n1. Read question carefully for exact provisioner name\n2. VolumeBindingMode is often specified in question requirements\n3. Only one StorageClass can be default at a time\n4. Don't modify existing resources unless explicitly asked\n5. Verify default status with kubectl get storageclass\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get storageclass\n• kubectl describe storageclass NAME\n• kubectl get storageclass NAME -o yaml\n• kubectl get pvc (check which StorageClass is used)\n• kubectl get events (check provisioning events)\n\n📋 STORAGECLASS FIELDS REFERENCE:\n\n**Required Fields:**\n- apiVersion: storage.k8s.io/v1\n- kind: StorageClass\n- metadata.name: StorageClass name\n- provisioner: Provisioner identifier\n\n**Important Optional Fields:**\n- volumeBindingMode: Immediate | WaitForFirstConsumer\n- reclaimPolicy: Delete | Retain | Recycle\n- allowVolumeExpansion: true | false\n- parameters: Provisioner-specific parameters\n\n**Default StorageClass:**\n- annotations.storageclass.kubernetes.io/is-default-class: \"true\"\n\n🎯 PROVISIONER EXAMPLES:\n\n**Cloud Providers:**\n- kubernetes.io/aws-ebs (AWS EBS)\n- kubernetes.io/gce-pd (Google Persistent Disk)\n- kubernetes.io/azure-disk (Azure Disk)\n\n**Local Storage:**\n- kubernetes.io/no-provisioner (Static local)\n- rancher.io/local-path (Dynamic local)\n\n**CSI Drivers:**\n- ebs.csi.aws.com (AWS EBS CSI)\n- pd.csi.storage.gke.io (Google PD CSI)\n\n🔧 VOLUME BINDING MODE COMPARISON:\n\n**Immediate:**\n- ✅ Simple, immediate provisioning\n- ❌ May create PV on wrong node\n- ❌ Pod scheduling may fail\n\n**WaitForFirstConsumer:**\n- ✅ Optimal node placement\n- ✅ Better resource utilization\n- ✅ Prevents scheduling conflicts\n- ❌ Slightly more complex flow"
  },
  {
    "id": "Q7",
    "title": "HorizontalPodAutoscaler (HPA)",
    "category": "Workloads & Scheduling",
    "difficulty": "Medium",
    "task": "Task:\nCreate a new HorizontalPodAutoscaler (HPA) named nginx-server in the autoscale namespace. This HPA must target the existing Deployment named nginx-server in the autoscale namespace.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) Metrics-server already installed and configured\nkubectl get pods -n kube-system | grep metrics-server  # metrics-server running\nkubectl top nodes  # metrics collection working\n\n# 2) Namespace and deployment already exist\nkubectl get ns autoscale  # namespace exists\nkubectl get deployment -n autoscale nginx-server  # deployment exists with resource limits\n\n# 3) Deployment has proper resource requests/limits for HPA\n# CPU requests: 100m, CPU limits: 200m (required for HPA functionality)",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Verify existing resources and metrics-server\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n autoscale nginx-server\nNAME           READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-server   1/1     1            1           5m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl top nodes\nNAME              CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nec2-34-201-252-187.compute-1.amazonaws.com   131m         6%     1437Mi          38%       \nec2-54-144-18-63.compute-1.amazonaws.com   155m         7%     1106Mi          29%       \n\n# 3) Create HPA using kubectl autoscale command (fastest method)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl autoscale deployment nginx-server --cpu-percent=50 --min=1 --max=4 -n autoscale\nhorizontalpodautoscaler.autoscaling/nginx-server autoscaled\n\n# 4) Verify initial HPA creation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get hpa -n autoscale\nNAME           REFERENCE                 TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nnginx-server   Deployment/nginx-server   <unknown>/50%   1         4         1          5s\n\n# 5) Add scale down stabilization window using YAML method\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ cat <<EOF | kubectl apply -f -\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-server\n  namespace: autoscale\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-server\n  minReplicas: 1\n  maxReplicas: 4\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 30\nEOF\n\nhorizontalpodautoscaler.autoscaling/nginx-server configured\n\n# Alternative method using kubectl edit:\n# [ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl edit hpa nginx-server -n autoscale\n# Add the following under spec:\n# behavior:\n#   scaleDown:\n#     stabilizationWindowSeconds: 30\n\n# 6) Verify HPA configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get hpa nginx-server -n autoscale\nNAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx-server   Deployment/nginx-server   0%/50%    1         4         1          45s\n\n# 7) Check detailed HPA configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe hpa nginx-server -n autoscale\nName:                                                  nginx-server\nNamespace:                                             autoscale\nReference:                                             Deployment/nginx-server\nMetrics:                                               ( current / target )\n  resource cpu on pods  (as a percentage of request):  0% (0) / 50%\nMin replicas:                                          1\nMax replicas:                                          4\nBehavior:\n  Scale Up:\n    Stabilization Window: 0 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Pods     Value: 4    Period: 15 seconds\n      - Type: Percent  Value: 100  Period: 15 seconds\n  Scale Down:\n    Stabilization Window: 30 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Percent  Value: 100  Period: 15 seconds\nDeployment pods:       1 current / 1 desired\nConditions:\n  Type            Status  Reason               Message\n  ----            ------  ------               -------\n  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation\n  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range\n\n# 8) Test HPA functionality (optional verification)\n# Create a service to test load\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl expose deployment nginx-server --port=80 -n autoscale\nservice/nginx-server exposed\n\n# Generate load to test scaling\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl run load-generator --image=busybox --restart=Never -n autoscale -- /bin/sh -c \"while true; do wget -q -O- http://nginx-server.autoscale.svc.cluster.local; done\"\n\n# Monitor HPA scaling\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get hpa nginx-server -n autoscale --watch\nNAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx-server   Deployment/nginx-server   0%/50%    1         4         1          2m\nnginx-server   Deployment/nginx-server   26%/50%   1         4         1          2m30s\nnginx-server   Deployment/nginx-server   53%/50%   1         4         1          3m\nnginx-server   Deployment/nginx-server   53%/50%   1         4         2          3m15s\n\n# Clean up load generator\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl delete pod load-generator -n autoscale\n\n# 9) Final verification\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get all -n autoscale\nNAME                                READY   STATUS    RESTARTS   AGE\npod/nginx-server-85f88d94f9-xxxxx   1/1     Running   0          10m\npod/nginx-server-85f88d94f9-yyyyy   1/1     Running   0          2m\n\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/nginx-server   ClusterIP   10.111.64.102   <none>        80/TCP    5m\n\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx-server   2/2     2            2           10m\n\nNAME                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/nginx-server-85f88d94f9   2         2         2       10m\n\nNAME                                               REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/nginx-server   Deployment/nginx-server   22%/50%   1         4         2          8m\n\n# 10) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ HPA \"nginx-server\" created in \"autoscale\" namespace\n# ✅ Target CPU utilization set to 50%\n# ✅ Min replicas: 1, Max replicas: 4 configured\n# ✅ Scale down stabilization window set to 30 seconds\n# ✅ HPA successfully targets nginx-server deployment\n# ✅ Scaling functionality verified (1 → 2 replicas under load)",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         HPA & Horizontal Pod Autoscaling                   │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │  Metrics-Server │    │      │\n│  │  │  - API Server   │    │              │  │      Pod        │    │      │\n│  │  │  - HPA Controller│    │              │  │                 │    │      │\n│  │  │  - kubectl CLI  │    │              │  │ ┌─────────────┐ │    │      │\n│  │  └─────────────────┘    │              │  │ │Metrics API  │ │    │      │\n│  └─────────────────────────┘              │  │ │Collection   │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           │                         │      │\n│                                           │  ┌─────────────────┐    │      │\n│                                           │  │  nginx Pods     │    │      │\n│                                           │  │ (autoscale ns)  │    │      │\n│                                           │  │                 │    │      │\n│                                           │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │nginx-server │ │    │      │\n│                                           │  │ │CPU: 100m req│ │    │      │\n│                                           │  │ │CPU: 200m lim│ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  │                 │    │      │\n│                                           │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │nginx-server │ │    │      │\n│                                           │  │ │ (scaled)    │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │   HPA Layer         │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │      HPA      │  │\n                          │  │ nginx-server  │  │\n                          │  │               │  │\n                          │  │ Target: 50%   │  │\n                          │  │ Min: 1 pod    │  │\n                          │  │ Max: 4 pods   │  │\n                          │  │               │  │\n                          │  │ Scale Down:   │  │\n                          │  │ 30s window    │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │  Deployment   │  │\n                          │  │ nginx-server  │  │\n                          │  │               │  │\n                          │  │ Current: 2/2  │  │\n                          │  │ Desired: 2    │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              HPA Decision Flow:\n                    ┌─────────────────────────────────┐\n                    │    1. Metrics Collection        │\n                    │   (Every 15 seconds)           │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. CPU Utilization Check     │\n                    │   Current: 53% vs Target: 50%  │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. Scaling Decision          │\n                    │   53% > 50% → Scale Up         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Calculate Replicas        │\n                    │   ceil(1 * 53/50) = 2 pods     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    5. Update Deployment         │\n                    │   Replicas: 1 → 2              │\n                    └─────────────────────────────────┘\n\n                              Scaling Behavior:\n                    ┌─────────────────────────────────┐\n                    │         SCALE UP                │\n                    │                                 │\n                    │ Trigger: CPU > 50%              │\n                    │ Window: 0 seconds (immediate)   │\n                    │ Max increase: 100% or 4 pods    │\n                    │ Period: 15 seconds              │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         SCALE DOWN              │\n                    │                                 │\n                    │ Trigger: CPU < 50%              │\n                    │ Window: 30 seconds (stabilize)  │\n                    │ Max decrease: 100%              │\n                    │ Period: 15 seconds              │\n                    └─────────────────────────────────┘\n\n                              Metrics Flow:\n                    ┌─────────────────────────────────┐\n                    │         kubelet                 │\n                    │    (collects metrics)          │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │      metrics-server             │\n                    │   (aggregates metrics)         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │      HPA Controller             │\n                    │   (makes scaling decisions)    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │      Deployment                 │\n                    │   (updates replica count)      │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. HPA REQUIREMENTS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │Metrics-     │    │Resource     │    │Target       │\n   │Server       │    │Requests     │    │Deployment   │\n   │(Required)   │    │(Required)   │    │(Required)   │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. HPA VERSIONS (CRITICAL):\n   • autoscaling/v1: Basic CPU-based scaling only\n   • autoscaling/v2: Advanced metrics, behavior, multiple metrics\n   • autoscaling/v2beta2: Deprecated, use v2\n\n3. SCALING METRICS:\n   • Resource: CPU, Memory (percentage of requests)\n   • Pods: Custom metrics per pod\n   • Object: Custom metrics from Kubernetes objects\n   • External: External metrics (Prometheus, etc.)\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 8-12 minutes maximum\n   • Use kubectl autoscale for basic HPA creation (fastest)\n   • Use YAML for advanced configuration (behavior)\n   • Always verify metrics-server is working first\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Missing resource requests in deployment (HPA won't work)\n   ❌ Wrong API version (use autoscaling/v2 for behavior)\n   ❌ Incorrect target reference (must match deployment name)\n   ❌ Not verifying metrics-server is running\n   ❌ Forgetting to specify namespace\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl top nodes (verify metrics-server working)\n   ✅ kubectl get hpa -n NAMESPACE (check HPA status)\n   ✅ kubectl describe hpa NAME -n NAMESPACE (verify configuration)\n   ✅ kubectl get deployment -n NAMESPACE (check resource requests)\n   ✅ Generate load to test scaling (if time permits)\n\n4. QUICK CREATION WORKFLOW:\n   • Step 1: kubectl autoscale deployment NAME --cpu-percent=X --min=Y --max=Z\n   • Step 2: kubectl apply -f hpa.yaml (for behavior configuration)\n   • Step 3: kubectl get hpa (verify creation)\n   • Step 4: kubectl describe hpa (verify all settings)\n\n📚 WORKLOADS & SCHEDULING DOMAIN (15% of CKA):\n\n• HPA: Horizontal scaling based on metrics\n• VPA: Vertical scaling (resource requests/limits)\n• Resource Requests/Limits: Required for HPA functionality\n• Metrics Server: Provides resource metrics API\n• Custom Metrics: Prometheus, external metrics adapters\n\n🔧 TROUBLESHOOTING TIPS:\n\nHPA not working?\n→ Check: Metrics-server running, resource requests defined, correct target reference\n\nMetrics showing <unknown>?\n→ Check: Metrics-server pods, resource requests in deployment, wait for metrics collection\n\nHPA not scaling?\n→ Check: CPU threshold reached, stabilization windows, min/max replica limits\n\nScaling too aggressive?\n→ Check: Behavior configuration, stabilization windows, scaling policies\n\n💡 EXAM DAY REMINDERS:\n\n1. Verify metrics-server is running before creating HPA\n2. Deployment MUST have resource requests for HPA to work\n3. Use kubectl autoscale for quick creation, YAML for advanced features\n4. Behavior configuration requires autoscaling/v2 API version\n5. Test with kubectl top pods to see current resource usage\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl top nodes (verify metrics-server)\n• kubectl top pods -n NAMESPACE (check pod resource usage)\n• kubectl get hpa -n NAMESPACE (check HPA status)\n• kubectl describe hpa NAME -n NAMESPACE (detailed configuration)\n• kubectl get events -n NAMESPACE (check scaling events)\n\n📋 HPA YAML TEMPLATE:\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: HPA-NAME\n  namespace: NAMESPACE\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: DEPLOYMENT-NAME\n  minReplicas: MIN-REPLICAS\n  maxReplicas: MAX-REPLICAS\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: TARGET-PERCENTAGE\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: SECONDS\n    scaleUp:\n      stabilizationWindowSeconds: SECONDS\n```\n\n🎯 KUBECTL AUTOSCALE SYNTAX:\n\n```bash\nkubectl autoscale deployment DEPLOYMENT-NAME \\\n  --cpu-percent=TARGET-PERCENTAGE \\\n  --min=MIN-REPLICAS \\\n  --max=MAX-REPLICAS \\\n  -n NAMESPACE\n```\n\n🔧 SCALING CALCULATION:\n\n**Formula**: `desiredReplicas = ceil(currentReplicas * currentMetric / targetMetric)`\n\n**Example**:\n- Current replicas: 1\n- Current CPU: 80%\n- Target CPU: 50%\n- Desired replicas: ceil(1 * 80/50) = ceil(1.6) = 2\n\n🎯 BEHAVIOR CONFIGURATION:\n\n**Scale Up Policies**:\n- Type: Pods (absolute number)\n- Type: Percent (percentage increase)\n- Period: Time window for policy\n\n**Scale Down Policies**:\n- stabilizationWindowSeconds: Prevents flapping\n- selectPolicy: Max, Min, Disabled\n- Policies: Same as scale up\n\n**Common Stabilization Windows**:\n- Scale Up: 0-60 seconds (default: 0)\n- Scale Down: 60-300 seconds (default: 300)"
  },
  {
    "id": "Q8",
    "title": "PriorityClass",
    "category": "Workloads & Scheduling",
    "difficulty": "Medium",
    "task": "Task:\nPlease perform the following tasks:",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) Namespace and resources already exist\nkubectl get ns priority  # namespace exists\nkubectl get priorityclass  # existing PriorityClasses visible\nkubectl get deployment -n priority busybox-logger  # deployment exists\n\n# 2) Current PriorityClasses:\n# - system-cluster-critical: 2000000000 (system)\n# - system-node-critical: 2000001000 (system)  \n# - max-priority: 1000000000 (user-defined, highest)\n\n# 3) busybox:1.28 image will be pulled automatically",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Check existing PriorityClasses to find the highest user-defined value\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get priorityclass --sort-by=.value\nNAME                      VALUE        GLOBAL-DEFAULT   AGE\nhigh-priority             999999999    false            2m\nmax-priority              1000000000   false            5m\nsystem-cluster-critical   2000000000   false            75m\nsystem-node-critical      2000001000   false            75m\n\n# Analysis:\n# - system-cluster-critical and system-node-critical are system PriorityClasses\n# - max-priority (1000000000) is the highest user-defined priority class\n# - Need to create high-priority with value 999999999 (1000000000 - 1)\n\n# 3) Create the high-priority PriorityClass\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ vim priority.yaml\n\n# Press 'i' to enter insert mode, then create:\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 999999999\nglobalDefault: false\ndescription: 'High priority for user workloads - one less than max-priority'\n\n# Press Esc, then :wq to save and exit\n\n# 4) Apply the PriorityClass\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl apply -f priority.yaml\npriorityclass.scheduling.k8s.io/high-priority created\n\n# 5) Verify PriorityClass creation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get priorityclass high-priority\nNAME            VALUE       GLOBAL-DEFAULT   AGE\nhigh-priority   999999999   false            10s\n\n# 6) Check current busybox-logger deployment\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment busybox-logger -n priority\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nbusybox-logger   1/1     1            1           5m\n\n# 7) Edit the deployment to add priorityClassName\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl edit deployment busybox-logger -n priority\n\n# In the editor, find the spec.template.spec section and add priorityClassName:\n# spec:\n#   template:\n#     spec:\n#       priorityClassName: high-priority  # Add this line\n#       containers:\n#       - name: busybox\n#         image: busybox:1.28\n#         ...\n\n# Save and exit (:wq in vim)\n# Expected output: deployment.apps/busybox-logger edited\n\n# Alternative method using kubectl patch:\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl patch deployment busybox-logger -n priority -p '{\"spec\":{\"template\":{\"spec\":{\"priorityClassName\":\"high-priority\"}}}}'\n\n# 8) Verify the deployment update and rollout\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl rollout status deployment busybox-logger -n priority\nWaiting for deployment \"busybox-logger\" rollout to finish: 1 old replicas are pending termination...\ndeployment \"busybox-logger\" successfully rolled out\n\n# 9) Check the new pod has the correct priority\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n priority\nNAME                              READY   STATUS    RESTARTS   AGE\nbusybox-logger-7c9585b756-xxxxx   1/1     Running   0          30s\n\n# Get the running pod name and check its priority\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ POD_NAME=$(kubectl get pods -n priority -l app=busybox-logger --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe pod $POD_NAME -n priority | grep -E 'Priority|QoS'\nPriority:             999999999\nPriority Class Name:  high-priority\nQoS Class:            BestEffort\n\n# 10) Verify deployment configuration\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment busybox-logger -n priority -o jsonpath='{.spec.template.spec.priorityClassName}'\nhigh-priority\n\n# 11) Additional verification - check all pods with priority information\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n priority -o custom-columns='NAME:.metadata.name,PRIORITY:.spec.priority,PRIORITY_CLASS:.spec.priorityClassName'\nNAME                              PRIORITY    PRIORITY_CLASS\nbusybox-logger-7c9585b756-xxxxx   999999999   high-priority\n\n# 12) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ PriorityClass \"high-priority\" created with value 999999999\n# ✅ Value is one less than highest user-defined priority (max-priority: 1000000000)\n# ✅ busybox-logger deployment updated to use high-priority PriorityClass\n# ✅ Deployment successfully rolled out with new priority\n# ✅ New pod has correct priority (999999999) and priorityClassName (high-priority)\n# ✅ No other deployments in priority namespace were modified",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         PriorityClass & Pod Scheduling                     │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (master01)            │              │   (worker01)            │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │  busybox Pod    │    │      │\n│  │  │  - API Server   │    │              │  │ (priority ns)   │    │      │\n│  │  │  - Scheduler    │    │              │  │                 │    │      │\n│  │  │  - kubectl CLI  │    │              │  │ ┌─────────────┐ │    │      │\n│  │  └─────────────────┘    │              │  │ │   busybox   │ │    │      │\n│  └─────────────────────────┘              │  │ │Priority:    │ │    │      │\n│                                           │  │ │ 999999999   │ │    │      │\n│                                           │  │ │PriorityClass│ │    │      │\n│                                           │  │ │high-priority│ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │ PriorityClass Layer │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │system-node-   │  │\n                          │  │  critical     │  │\n                          │  │ 2000001000    │  │\n                          │  │  (HIGHEST)    │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │system-cluster-│  │\n                          │  │  critical     │  │\n                          │  │ 2000000000    │  │\n                          │  │   (SYSTEM)    │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │ max-priority  │  │\n                          │  │ 1000000000    │  │\n                          │  │ (USER HIGH)   │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │high-priority  │  │\n                          │  │  999999999    │  │\n                          │  │ (USER MEDIUM) │  │\n                          │  │      ✅       │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │   Default     │  │\n                          │  │      0        │  │\n                          │  │   (LOWEST)    │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Scheduling Priority:\n                    ┌─────────────────────────────────┐\n                    │    1. Pod Submission            │\n                    │   (with priorityClassName)     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. Priority Resolution       │\n                    │   high-priority → 999999999    │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. Scheduler Queue           │\n                    │   Higher priority pods first   │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Node Selection            │\n                    │   Best fit node chosen         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    5. Pod Preemption            │\n                    │   Lower priority pods evicted  │\n                    │   (if resources needed)        │\n                    └─────────────────────────────────┘\n\n                              Deployment Update Flow:\n                    ┌─────────────────────────────────┐\n                    │    1. Edit Deployment           │\n                    │   Add priorityClassName         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. Rolling Update            │\n                    │   New ReplicaSet created       │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. New Pod Creation          │\n                    │   With high-priority class     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Old Pod Termination       │\n                    │   Previous pod terminated       │\n                    └─────────────────────────────────┘\n\n                              Priority Comparison:\n                    ┌─────────────────────────────────┐\n                    │         BEFORE UPDATE          │\n                    │                                 │\n                    │ Pod Priority: 0 (default)      │\n                    │ PriorityClassName: <none>       │\n                    │ Scheduling: Normal queue        │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         AFTER UPDATE           │\n                    │                                 │\n                    │ Pod Priority: 999999999         │\n                    │ PriorityClassName: high-priority│\n                    │ Scheduling: High priority queue │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. PRIORITYCLASS COMPONENTS:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │    Value    │    │GlobalDefault│    │Description  │\n   │ (Required)  │    │ (Optional)  │    │ (Optional)  │\n   │   Integer   │    │   Boolean   │    │   String    │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. PRIORITY RANGES (CRITICAL):\n   • System: 2000000000+ (reserved for system components)\n   • User: 0-1999999999 (available for user workloads)\n   • Default: 0 (when no priorityClassName specified)\n   • Negative: Not allowed\n\n3. SCHEDULING BEHAVIOR:\n   • Higher priority pods scheduled first\n   • Preemption: Higher priority can evict lower priority\n   • Queue ordering: Priority-based scheduling queue\n   • Resource contention: Priority determines resource allocation\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 6-10 minutes maximum\n   • Check existing PriorityClasses first\n   • Calculate value correctly (highest user-defined - 1)\n   • Use kubectl patch for quick deployment updates\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Using system priority range (2000000000+)\n   ❌ Wrong value calculation (not one less than highest)\n   ❌ Incorrect priorityClassName in deployment\n   ❌ Modifying wrong deployment or other deployments\n   ❌ Not verifying the rollout completed\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get priorityclass --sort-by=.value (check priority order)\n   ✅ kubectl get deployment NAME -o yaml | grep priority (verify deployment)\n   ✅ kubectl describe pod POD-NAME | grep Priority (verify pod priority)\n   ✅ kubectl rollout status deployment NAME (verify rollout)\n\n4. EFFICIENT WORKFLOW:\n   • Step 1: kubectl get priorityclass --sort-by=.value\n   • Step 2: Identify highest user-defined value\n   • Step 3: Create PriorityClass with value - 1\n   • Step 4: kubectl patch deployment to add priorityClassName\n   • Step 5: Verify rollout and pod priority\n\n📚 WORKLOADS & SCHEDULING DOMAIN (15% of CKA):\n\n• PriorityClass: Defines scheduling priority for pods\n• Pod Priority: Numeric value determining scheduling order\n• Preemption: Higher priority pods can evict lower priority\n• Scheduler: Uses priority for queue ordering and resource allocation\n• QoS Classes: BestEffort, Burstable, Guaranteed (separate from priority)\n\n🔧 TROUBLESHOOTING TIPS:\n\nPriorityClass not working?\n→ Check: Correct value range, valid YAML syntax, applied successfully\n\nPod not getting priority?\n→ Check: priorityClassName in deployment spec, pod recreated after update\n\nDeployment not updating?\n→ Check: Correct namespace, deployment name, rollout status\n\nPriority not showing in pod?\n→ Check: Pod is new (created after deployment update), correct pod name\n\n💡 EXAM DAY REMINDERS:\n\n1. Always check existing PriorityClasses first with --sort-by=.value\n2. Calculate \"one less\" correctly (subtract 1 from highest user-defined)\n3. Use kubectl patch for quick deployment updates\n4. Verify pod priority with kubectl describe pod\n5. Don't modify system PriorityClasses or other deployments\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl get priorityclass --sort-by=.value\n• kubectl describe priorityclass NAME\n• kubectl get deployment NAME -o yaml | grep priority\n• kubectl describe pod POD-NAME | grep -E 'Priority|QoS'\n• kubectl rollout status deployment NAME\n\n📋 PRIORITYCLASS YAML TEMPLATE:\n\n```yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: PRIORITY-CLASS-NAME\nvalue: PRIORITY-VALUE\nglobalDefault: false\ndescription: 'Description of priority class'\n```\n\n🎯 KUBECTL PATCH SYNTAX:\n\n```bash\nkubectl patch deployment DEPLOYMENT-NAME -n NAMESPACE \\\n  -p '{\"spec\":{\"template\":{\"spec\":{\"priorityClassName\":\"PRIORITY-CLASS-NAME\"}}}}'\n```\n\n🔧 PRIORITY VALUE GUIDELINES:\n\n**System Reserved**: 2000000000 and above\n- system-node-critical: 2000001000\n- system-cluster-critical: 2000000000\n\n**User Range**: 0 to 1999999999\n- High priority: 1000000000+\n- Medium priority: 100000000+\n- Low priority: 1000000+\n- Default: 0\n\n**Calculation Example**:\n- Existing max-priority: 1000000000\n- Required high-priority: 999999999 (1000000000 - 1)\n\n🎯 PREEMPTION BEHAVIOR:\n\n**When Resources Scarce**:\n1. Higher priority pod submitted\n2. No available resources on any node\n3. Scheduler identifies lower priority pods to evict\n4. Lower priority pods terminated\n5. Higher priority pod scheduled\n\n**Preemption Policy**:\n- PreemptLowerPriority (default): Can preempt lower priority\n- Never: Cannot preempt any pods"
  },
  {
    "id": "Q9",
    "title": "Resource Management",
    "category": "Workloads & Scheduling",
    "difficulty": "Hard",
    "task": "Context:\nYou manage a WordPress application. Due to excessive resource requests, some Pods cannot start.",
    "environment": "- Cluster: 2-node Kubernetes v1.28.15 (master01 + worker01)\n- Master node: ec2-34-201-252-187.compute-1.amazonaws.com (34.201.252.187, Private: 172.31.80.10)\n- Worker node: ec2-54-144-18-63.compute-1.amazonaws.com (54.144.18.63, Private: 172.31.80.11)\n- Container runtime: containerd\n- CNI: Calico + Flannel",
    "preparation": "# 1) Namespace and deployment already exist\nkubectl get ns relative-fawn  # namespace exists\nkubectl get deployment -n relative-fawn wordpress  # deployment with high resource requests\n\n# 2) Current resource allocation shows scheduling issues\nkubectl get pods -n relative-fawn  # some pods pending due to insufficient resources\n\n# 3) Node capacity: Worker node has ~2000m CPU, ~4000Mi Memory available\n# Current allocation: ~55% CPU, ~44% Memory after optimization",
    "steps": "# 1) Connect to master node (exam will specify which node to use)\n[candidate@base ~]$ ssh ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$\n\n# 2) Check current deployment status and identify resource issues\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n relative-fawn wordpress\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nwordpress   1/3     3            1           2m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n relative-fawn\nNAME                        READY   STATUS    RESTARTS   AGE\nwordpress-66767c94c-2jg76   1/1     Running   0          2m\nwordpress-66767c94c-hsrbr   0/1     Pending   0          2m\nwordpress-66767c94c-xdtb2   0/1     Pending   0          2m\n\n# Check why pods are pending\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe pod wordpress-66767c94c-hsrbr -n relative-fawn | grep Events -A 5\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  2m    default-scheduler  0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had untolerated taint\n\n# 3) Scale WordPress deployment to 0 replicas (required before resource changes)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl scale deployment wordpress --replicas=0 -n relative-fawn\ndeployment.apps/wordpress scaled\n\n# Verify scaling down\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n relative-fawn wordpress\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nwordpress   0/0     0            0           3m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n relative-fawn\nNo resources found in relative-fawn namespace.\n\n# 4) Analyze node resource capacity\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl top nodes\nNAME              CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nec2-34-201-252-187.compute-1.amazonaws.com   160m         8%     1459Mi          39%       \nec2-54-144-18-63.compute-1.amazonaws.com   124m         6%     1446Mi          38%       \n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe node ec2-54-144-18-63.compute-1.amazonaws.com | grep -A 10 \"Allocated resources\"\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                650m (32%)    200m (10%)\n  memory             468Mi (12%)   256Mi (6%)\n\n# Analysis: Worker node has ~68% CPU and ~88% Memory available\n# For 3 pods with 2 containers each (6 containers total):\n# Available: ~1360m CPU, ~3520Mi Memory\n# Per container: ~226m CPU, ~586Mi Memory\n# Conservative allocation: 150m CPU, 400Mi Memory per container\n\n# 5) Update deployment resource requests\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl edit deployment wordpress -n relative-fawn\n\n# In the editor, update both initContainers and containers sections:\n# Change from:\n#   resources:\n#     limits:\n#       cpu: 1000m\n#       memory: 1200Mi\n# To:\n#   resources:\n#     requests:        # Add this section\n#       cpu: 150m\n#       memory: 400Mi\n#     limits:          # Keep existing limits\n#       cpu: 1000m\n#       memory: 1200Mi\n\n# Complete updated spec should look like:\nspec:\n  template:\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: busybox:1.28\n        command: ['sh', '-c', 'sleep 6']\n        resources:\n          requests:      # Add requests section\n            cpu: 150m\n            memory: 400Mi\n          limits:        # Keep existing limits\n            cpu: 1000m\n            memory: 1200Mi\n      containers:\n      - name: wordpress\n        image: wordpress:php8.0-apache\n        ports:\n        - containerPort: 80\n        resources:\n          requests:      # Add requests section\n            cpu: 150m\n            memory: 400Mi\n          limits:        # Keep existing limits\n            cpu: 1000m\n            memory: 1200Mi\n\n# Save and exit (:wq in vim)\n# Expected output: deployment.apps/wordpress edited\n\n# Alternative method using kubectl patch:\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl patch deployment wordpress -n relative-fawn -p '{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"initContainers\": [\n          {\n            \"name\": \"init-mysql\",\n            \"resources\": {\n              \"requests\": {\n                \"cpu\": \"150m\",\n                \"memory\": \"400Mi\"\n              }\n            }\n          }\n        ],\n        \"containers\": [\n          {\n            \"name\": \"wordpress\",\n            \"resources\": {\n              \"requests\": {\n                \"cpu\": \"150m\",\n                \"memory\": \"400Mi\"\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}'\n\n# 6) Scale WordPress deployment back to 3 replicas\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl scale deployment wordpress --replicas=3 -n relative-fawn\ndeployment.apps/wordpress scaled\n\n# 7) Verify deployment and pod status\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get deployment -n relative-fawn wordpress\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nwordpress   3/3     3            3           5m\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get pods -n relative-fawn\nNAME                         READY   STATUS    RESTARTS   AGE\nwordpress-679f96dc79-4dmds   1/1     Running   0          1m\nwordpress-679f96dc79-7nsz8   1/1     Running   0          1m\nwordpress-679f96dc79-fhmrb   1/1     Running   0          1m\n\n# 8) Verify resource requests are applied correctly\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ POD_NAME=$(kubectl get pods -n relative-fawn -l app=wordpress -o jsonpath='{.items[0].metadata.name}')\n\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe pod $POD_NAME -n relative-fawn | grep -A 5 \"Requests:\"\n    Requests:\n      cpu:        150m\n      memory:     400Mi\n    Environment:  <none>\n    Mounts:\n--\n    Requests:\n      cpu:        150m\n      memory:     400Mi\n    Environment:  <none>\n    Mounts:\n\n# 9) Check updated node resource allocation\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl describe node ec2-54-144-18-63.compute-1.amazonaws.com | grep -A 10 \"Allocated resources\"\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1100m (55%)   3200m (160%)\n  memory             1668Mi (44%)  3856Mi (103%)\n\n# Verification: Node now has healthy resource utilization with room for stability\n\n# 10) Final verification of all resources\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ kubectl get all -n relative-fawn\nNAME                             READY   STATUS    RESTARTS   AGE\npod/wordpress-679f96dc79-4dmds   1/1     Running   0          2m\npod/wordpress-679f96dc79-7nsz8   1/1     Running   0          2m\npod/wordpress-679f96dc79-fhmrb   1/1     Running   0          2m\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/wordpress   3/3     3            3           6m\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/wordpress-679f96dc79   3         3         3       2m\n\n# 11) Exit back to base node (if exam requires)\n[ubuntu@ec2-34-201-252-187.compute-1.amazonaws.com ~]$ exit\n[candidate@base ~]$\n\n# Task completed successfully:\n# ✅ WordPress deployment scaled to 0 replicas before resource changes\n# ✅ Resource requests added: 150m CPU, 400Mi Memory per container\n# ✅ Same requests applied to both init containers and main containers\n# ✅ Resource limits preserved unchanged\n# ✅ Node resources distributed evenly among 3 pods (6 containers total)\n# ✅ All 3 replicas running and ready\n# ✅ Node utilization healthy: 55% CPU, 44% Memory with sufficient overhead",
    "diagram": "┌─────────────────────────────────────────────────────────────────────────────┐\n│                          AWS Kubernetes Cluster                            │\n│                         Resource Management & Optimization                 │\n│                                                                             │\n│  ┌─────────────────────────┐              ┌─────────────────────────┐      │\n│  │      Master Node        │              │      Worker Node        │      │\n│  │   ec2-34-201-252-187.compute-1.amazonaws.com       │              │   ec2-54-144-18-63.compute-1.amazonaws.com       │      │\n│  │   (control-plane)       │              │   (schedulable)         │      │\n│  │                         │              │                         │      │\n│  │  ┌─────────────────┐    │              │  ┌─────────────────┐    │      │\n│  │  │  Control Plane  │    │              │  │ WordPress Pods  │    │      │\n│  │  │  - API Server   │    │              │  │(relative-fawn)  │    │      │\n│  │  │  - Scheduler    │    │              │  │                 │    │      │\n│  │  │  - kubectl CLI  │    │              │  │ ┌─────────────┐ │    │      │\n│  │  └─────────────────┘    │              │  │ │   Pod 1     │ │    │      │\n│  │                         │              │  │ │init: 150m   │ │    │      │\n│  │  ┌─────────────────┐    │              │  │ │main: 150m   │ │    │      │\n│  │  │   Not Used for  │    │              │  │ │Total:300m   │ │    │      │\n│  │  │   Workloads     │    │              │  │ └─────────────┘ │    │      │\n│  │  │  (Tainted)      │    │              │  │                 │    │      │\n│  │  └─────────────────┘    │              │  │ ┌─────────────┐ │    │      │\n│  └─────────────────────────┘              │  │ │   Pod 2     │ │    │      │\n│                                           │  │ │init: 150m   │ │    │      │\n│                                           │  │ │main: 150m   │ │    │      │\n│                                           │  │ │Total:300m   │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  │                 │    │      │\n│                                           │  │ ┌─────────────┐ │    │      │\n│                                           │  │ │   Pod 3     │ │    │      │\n│                                           │  │ │init: 150m   │ │    │      │\n│                                           │  │ │main: 150m   │ │    │      │\n│                                           │  │ │Total:300m   │ │    │      │\n│                                           │  │ └─────────────┘ │    │      │\n│                                           │  └─────────────────┘    │      │\n│                                           └─────────────────────────┘      │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                                    ▼\n                          ┌─────────────────────┐\n                          │ Resource Allocation │\n                          │                     │\n                          │  ┌───────────────┐  │\n                          │  │ Worker Node   │  │\n                          │  │   Capacity    │  │\n                          │  │               │  │\n                          │  │ CPU: 2000m    │  │\n                          │  │ Memory: 4000Mi│  │\n                          │  │               │  │\n                          │  │ System: ~450m │  │\n                          │  │ Available:    │  │\n                          │  │ ~1550m CPU    │  │\n                          │  │ ~3550Mi Mem   │  │\n                          │  └───────────────┘  │\n                          │          │          │\n                          │          ▼          │\n                          │  ┌───────────────┐  │\n                          │  │ WordPress     │  │\n                          │  │ Allocation    │  │\n                          │  │               │  │\n                          │  │ 3 Pods × 2    │  │\n                          │  │ Containers    │  │\n                          │  │ = 6 Total     │  │\n                          │  │               │  │\n                          │  │ Per Container:│  │\n                          │  │ 150m CPU      │  │\n                          │  │ 400Mi Memory  │  │\n                          │  │               │  │\n                          │  │ Total Used:   │  │\n                          │  │ 900m CPU      │  │\n                          │  │ 2400Mi Memory │  │\n                          │  └───────────────┘  │\n                          └─────────────────────┘\n\n                              Resource Optimization Flow:\n                    ┌─────────────────────────────────┐\n                    │    1. Initial State             │\n                    │   High resource requests        │\n                    │   (800m CPU, 1Gi Memory)       │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    2. Scheduling Failure        │\n                    │   Insufficient resources        │\n                    │   Pods stuck in Pending         │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    3. Scale to Zero             │\n                    │   Temporary shutdown            │\n                    │   Prepare for resource update   │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    4. Resource Analysis         │\n                    │   Calculate available capacity  │\n                    │   Distribute evenly among pods  │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    5. Update Requests           │\n                    │   150m CPU, 400Mi Memory        │\n                    │   Same for init & main          │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │    6. Scale Back Up             │\n                    │   3 replicas successfully       │\n                    │   All pods running              │\n                    └─────────────────────────────────┘\n\n                              Before vs After:\n                    ┌─────────────────────────────────┐\n                    │         BEFORE (FAILED)        │\n                    │                                 │\n                    │ Per Container:                  │\n                    │ - CPU Request: 800m             │\n                    │ - Memory Request: 1Gi           │\n                    │                                 │\n                    │ Total for 6 containers:         │\n                    │ - CPU: 4800m (240% of node)    │\n                    │ - Memory: 6Gi (150% of node)   │\n                    │                                 │\n                    │ Result: Scheduling failures     │\n                    └─────────────────────────────────┘\n                                    │\n                                    ▼\n                    ┌─────────────────────────────────┐\n                    │         AFTER (SUCCESS)        │\n                    │                                 │\n                    │ Per Container:                  │\n                    │ - CPU Request: 150m             │\n                    │ - Memory Request: 400Mi         │\n                    │                                 │\n                    │ Total for 6 containers:         │\n                    │ - CPU: 900m (45% of node)      │\n                    │ - Memory: 2400Mi (60% of node) │\n                    │                                 │\n                    │ Result: All pods running        │\n                    └─────────────────────────────────┘\n\n                              Container Resource Pattern:\n                    ┌─────────────────────────────────┐\n                    │           Pod Structure         │\n                    │                                 │\n                    │  ┌─────────────────────────┐    │\n                    │  │    Init Container       │    │\n                    │  │    (init-mysql)         │    │\n                    │  │                         │    │\n                    │  │  Requests:              │    │\n                    │  │  - CPU: 150m            │    │\n                    │  │  - Memory: 400Mi        │    │\n                    │  │                         │    │\n                    │  │  Limits:                │    │\n                    │  │  - CPU: 1000m           │    │\n                    │  │  - Memory: 1200Mi       │    │\n                    │  └─────────────────────────┘    │\n                    │              │                  │\n                    │              ▼                  │\n                    │  ┌─────────────────────────┐    │\n                    │  │    Main Container       │    │\n                    │  │    (wordpress)          │    │\n                    │  │                         │    │\n                    │  │  Requests:              │    │\n                    │  │  - CPU: 150m            │    │\n                    │  │  - Memory: 400Mi        │    │\n                    │  │                         │    │\n                    │  │  Limits:                │    │\n                    │  │  - CPU: 1000m           │    │\n                    │  │  - Memory: 1200Mi       │    │\n                    │  └─────────────────────────┘    │\n                    └─────────────────────────────────┘",
    "tips": "🎯 CRITICAL EXAM CONCEPTS:\n\n1. RESOURCE TYPES:\n   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n   │  Requests   │    │   Limits    │    │    QoS      │\n   │(Scheduling) │    │(Enforcement)│    │ (Priority)  │\n   │ Guaranteed  │    │ Maximum     │    │ Classes     │\n   └─────────────┘    └─────────────┘    └─────────────┘\n\n2. RESOURCE UNITS (CRITICAL):\n   • CPU: millicores (m) - 1000m = 1 CPU core\n   • Memory: bytes (Ki, Mi, Gi) - 1024-based\n   • Ephemeral Storage: bytes (Ki, Mi, Gi)\n\n3. QOS CLASSES:\n   • Guaranteed: requests = limits for all containers\n   • Burstable: requests < limits or only requests specified\n   • BestEffort: no requests or limits specified\n\n🚀 EXAM SUCCESS STRATEGIES:\n\n1. TIME MANAGEMENT:\n   • This question: 10-15 minutes maximum\n   • Scale to 0 first (critical step)\n   • Calculate resources based on node capacity\n   • Use kubectl patch for quick updates\n\n2. COMMON MISTAKES TO AVOID:\n   ❌ Not scaling to 0 before resource changes\n   ❌ Different requests for init vs main containers\n   ❌ Modifying limits when only requests needed\n   ❌ Over-allocating resources (causing scheduling failures)\n   ❌ Under-allocating resources (causing instability)\n\n3. VERIFICATION CHECKLIST:\n   ✅ kubectl get deployment (check READY: 3/3)\n   ✅ kubectl get pods (all Running status)\n   ✅ kubectl describe pod | grep Requests (verify resource values)\n   ✅ kubectl top nodes (check node utilization)\n   ✅ kubectl describe node | grep Allocated (check resource allocation)\n\n4. RESOURCE CALCULATION WORKFLOW:\n   • Step 1: kubectl describe node (check available resources)\n   • Step 2: Calculate total containers (pods × containers per pod)\n   • Step 3: Divide available resources by container count\n   • Step 4: Add safety margin (10-20% overhead)\n   • Step 5: Apply same values to init and main containers\n\n📚 WORKLOADS & SCHEDULING DOMAIN (15% of CKA):\n\n• Resource Requests: Minimum guaranteed resources\n• Resource Limits: Maximum allowed resources\n• Node Capacity: Total available resources per node\n• Resource Quotas: Namespace-level resource constraints\n• LimitRanges: Default and maximum resource values\n\n🔧 TROUBLESHOOTING TIPS:\n\nPods stuck in Pending?\n→ Check: Node capacity, resource requests, node selectors, taints/tolerations\n\nInsufficient resources error?\n→ Check: kubectl describe node, total requests vs capacity\n\nOOMKilled pods?\n→ Check: Memory limits too low, increase memory requests/limits\n\nCPU throttling?\n→ Check: CPU limits too low, monitor with kubectl top pods\n\n💡 EXAM DAY REMINDERS:\n\n1. Always scale deployment to 0 before changing resource requests\n2. Apply same resource requests to init containers and main containers\n3. Don't modify limits unless specifically asked\n4. Leave 10-20% overhead for node stability\n5. Verify all pods are Running after scaling back up\n\n🔍 DEBUGGING COMMANDS:\n\n• kubectl top nodes (current resource usage)\n• kubectl describe node NODE-NAME (capacity and allocation)\n• kubectl get pods -o wide (pod placement)\n• kubectl describe pod POD-NAME (resource configuration)\n• kubectl get events --sort-by='.lastTimestamp' (scheduling events)\n\n📋 RESOURCE CALCULATION FORMULA:\n\n```\nAvailable CPU = Node Capacity - System Reserved - Current Allocation\nAvailable Memory = Node Capacity - System Reserved - Current Allocation\n\nPer Container CPU = (Available CPU × Safety Factor) / Total Containers\nPer Container Memory = (Available Memory × Safety Factor) / Total Containers\n\nSafety Factor = 0.8 (leave 20% overhead)\n```\n\n🎯 KUBECTL PATCH SYNTAX FOR RESOURCES:\n\n```bash\nkubectl patch deployment DEPLOYMENT-NAME -n NAMESPACE -p '{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"initContainers\": [\n          {\n            \"name\": \"INIT-CONTAINER-NAME\",\n            \"resources\": {\n              \"requests\": {\n                \"cpu\": \"CPU-VALUE\",\n                \"memory\": \"MEMORY-VALUE\"\n              }\n            }\n          }\n        ],\n        \"containers\": [\n          {\n            \"name\": \"MAIN-CONTAINER-NAME\",\n            \"resources\": {\n              \"requests\": {\n                \"cpu\": \"CPU-VALUE\",\n                \"memory\": \"MEMORY-VALUE\"\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}'\n```\n\n🔧 RESOURCE BEST PRACTICES:\n\n**Requests (Scheduling)**:\n- Set based on actual usage patterns\n- Include startup resource needs\n- Consider init container requirements\n- Leave room for node overhead\n\n**Limits (Enforcement)**:\n- Prevent resource hogging\n- Set higher than requests for burstability\n- Monitor for throttling/OOM kills\n- Consider application behavior\n\n**Node Utilization**:\n- Target 70-80% for production\n- Leave overhead for system processes\n- Monitor with kubectl top nodes\n- Plan for peak usage scenarios"
  }
]